<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[inotify+rsync实现数据实时同步]]></title>
    <url>%2F2017%2F12%2F17%2Finotify-rsync%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[数据实时同步介绍什么是实时同步：如何实现实时同步A. 要利用监控服务（inotify），监控同步数据服务器目录中信息的变化 B. 发现目录中数据产生变化，就利用rsync服务推送到备份服务器上 实现实时同步的方法inotify+rsync 方式实现数据同步 sersync 方式实现实时数据同步 实时同步原理介绍 inotify+rsync 方式实现数据同步Inotify简介Inotify是一种强大的，细粒度的。异步的文件系统事件监控机制，linux内核从2.6.13起，加入了Inotify支持，通过Inotify可以监控文件系统中添加、删除，修改、移动等各种事件,利用这个内核接口，第三方软件就可以监控文件系统下文件的各种变化情况，而 inotify-tools 正是实施这样监控的软件。 国人周洋在金山公司也开发了类似的实时同步软件sersync。 提示信息： sersync软件实际上就是在 inotify软件基础上进行开发的，功能要更加强大些 ，多了定时重传机制，过滤机制了提供接口做 CDN，支持多线程橾作。 Inotify实际是一种事件驱动机制，它为应用程序监控文件系统事件提供了实时响应事件的机制，而无须通过诸如cron等的轮询机制来获取事件。 cron等机制不仅无法做到实时性，而且消耗大量系统资源。相比之下，inotify基于事件驱动，可以做到对事件处理的实时响应，也没有轮询造成的系统资源消耗，是非常自然的事件通知接口，也与自然世界事件机制相符合。 inotify的实现有几款软件： inotify-tools，sersync，lrsyncd inotify+rsync使用方式inotify 对同步数据目录信息的监控 rsync 完成对数据信息的实时同步 利用脚本进行结合 部署inotify软件的前提需要2.6.13以后内核版本才能支持inotify软件。2.6.13内核之后版本，在没有安装inotify软件之前，应该有这三个文件。 123456789[root@backup ~]# ll /proc/sys/fs/inotify/total 0-rw-r--r-- 1 root root 0 Oct 17 10:12 max_queued_events-rw-r--r-- 1 root root 0 Oct 17 10:12 max_user_instances-rw-r--r-- 1 root root 0 Oct 17 10:12 max_user_watches 三个重要文件的说明 【服务优化】 可以将三个文件的数值调大，监听更大的范围 inotify软件介绍及参数说明两种安装方式1234- yum install -y inotify-tools- 手工编译安装 手工编译安装方式需要到github上进行下载软件包 inotify软件的参考资料链接： 1https://github.com/rvoicilas/inotify-tools/wiki inotify主要安装的两个软件inotifywait： （主要） 在被监控的文件或目录上等待特定文件系统事件（open close delete等）发生，执行后处于阻塞状态，适合在shell脚本中使用 inotifywatch： 收集被监控的文件系统使用的统计数据，指文件系统事件发生的次数统计。 说明：在实时实时同步的时候，主要是利用inotifywait对目录进行监控 inotifywait命令参数说明 -e[参数] 可以指定的事件类型inotifywait监控中的事件测试 创建事件 12345[root@nfs01 data]# touch test2.txt[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e create17-10-17 11:19 /data/test2.txt 事件信息: CREATE 删除事件 12345[root@nfs01 data]# \rm -f test1.txt[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e delete17-10-17 11:28 /data/test1.txt 事件信息: DELETE 修改事件 12345[root@nfs01 data]# echo &quot;132&quot; &gt; test.txt[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e close_write17-10-17 11:30 /data/test.txt 事件信息: CLOSE_WRITE,CLOSE 移动事件 moved_to 12345[root@nfs01 data]# mv /etc/hosts .[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e moved_to17-10-17 11:33 /data/hosts 事件信息: MOVED_TO 移动事件 moved_from 12345[root@nfs01 data]# mv ./hosts /tmp/[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e moved_from17-10-17 11:34 /data/hosts 事件信息: MOVED_FROM 修改输出的日期格式123[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d/%m/%y %H:%M&quot; --format &quot;%T %w%f&quot;17/10/17 11:12 /data/test1.txt 对inotifywait命令的测试对inotifywait命令测试的说明： 需要打开两个连接窗口 窗口运行inotifywait 窗口对文件夹进行操作，可在一窗口中查看出inotifywait的监控记录 创建文件的逻辑1234567891011121314151617[root@nfs01 ~]# inotifywait /dataSetting up watches.Watches established./data/ CREATE test1.txt/data/ OPEN test1.txt/data/ ATTRIB test1.txt/data/ CLOSE_WRITE,CLOSE test1.txt创建文件，inotifywait显示创建文件的过程↑[root@nfs01 data]# touch test1.txt 创建目录逻辑12345[root@nfs01 data]# mkdir testdir[root@nfs01 ~]#/data/ CREATE,ISDIR testdir 监控子目录下的文件123456789[root@nfs01 data]# touch testdir/test01.txt[root@nfs01 ~]# inotifywait -mrq /data/data/testdir/ OPEN test01.txt/data/testdir/ ATTRIB test01.txt/data/testdir/ CLOSE_WRITE,CLOSE test01.txt sed命令修改逻辑123456789101112131415161718192021222324252627[root@nfs01 data]# sed &apos;s#132#123#g&apos; test.txt -i[root@nfs01 ~]# inotifywait -mrq /data --timefmt &quot;%d-%m-%y %H:%M&quot; --format &quot;%T %w%f 事件信息: %e&quot; -e moved_from /data/test.txt 事件信息: OPEN /data/sedDh5R8v 事件信息: CREATE /data/sedDh5R8v 事件信息: OPEN /data/test.txt 事件信息: ACCESS /data/sedDh5R8v 事件信息: MODIFY /data/sedDh5R8v 事件信息: ATTRIB /data/sedDh5R8v 事件信息: ATTRIB /data/test.txt 事件信息: CLOSE_NOWRITE,CLOSE /data/sedDh5R8v 事件信息: CLOSE_WRITE,CLOSE /data/sedDh5R8v 事件信息: MOVED_FROM /data/test.txt 事件信息: MOVED_TO 实时同步命令参数示意图 inotify+rsync实时同步服务部署第一个里程碑：部署rsync服务rsync服务端部署 软件是否存在 123[root@backup ~]# rpm -qa |grep rsyncrsync-3.0.6-12.el6.x86_64 需求：查询到某个命令非常有用。但是不知道属于哪个软件包 123yum provides rysncprovides Find what package provides the given value 进行软件服务配置 1234567891011121314151617181920212223242526272829303132333435363738394041[root@backup ~]# vim /etc/rsyncd.confuid = rsyncgid = rsyncuse chroot = nomax connections = 200timeout = 300pid file = /var/run/rsyncd.pidlock file = /var/run/rsync.locklog file = /var/log/rsyncd.logignore errorsread only = falselist = falsehosts allow = 172.16.1.0/24auth users = rsync_backupsecrets file = /etc/rsync.password[backup]comment = &quot;backup dir by oldboy&quot;path = /backup[nfsbackup]comment = &quot;nfsbackup dir by hzs&quot;path = /nfsbackup 创建rsync管理用户 1[root@backup ~]# useradd -s /sbin/nologin -M rsync 创建数据备份储存目录,目录修改属主 123[root@backup ~]# mkdir /nfsbackup/[root@backup ~]# chown -R rsync.rsync /nfsbackup/ 创建认证用户密码文件并进行授权600 123echo &quot;rsync_backup:oldboy123&quot; &gt;&gt;/etc/rsync.passwordchmod 600 /etc/rsync.password 启动rsync服务 1rsync --daemon 至此服务端配置完成 12345[root@backup ~]# ps -ef |grep rsyncroot 2076 1 0 17:05 ? 00:00:00 rsync --daemonroot 2163 1817 0 17:38 pts/1 00:00:00 grep --color=auto rsync rsync客户端配置 软件是否存在 123[root@backup ~]# rpm -qa |grep rsyncrsync-3.0.6-12.el6.x86_64 创建安全认证文件，并进行修改权限600 123echo &quot;oldboy123&quot; &gt;&gt;/etc/rsync.passwordchmod 600 /etc/rsync.password 测试数据传输 1234567891011[root@nfs01 sersync]# rsync -avz /data rsync_backup@172.16.1.41::nfsbackup --password-file=/etc/rsync.passwordsending incremental file listdata/data/.hzsdata/.tar.gzdata/.txt 第二个里程碑：部署inotify服务安装inotify软件两种安装方式 123- yum install -y inotify-tools- 手工编译安装 注： 手工编译安装方式需要到github上进行下载软件包 inotify软件的参考资料链接： 1https://github.com/rvoicilas/inotify-tools/wiki 查看inotify安装上的两个命令 inotifywait inotifywatch 12345[root@nfs01 ~]# rpm -ql inotify-tools/usr/bin/inotifywait #主要/usr/bin/inotifywatch inotifywait和inotifywatch的作用 inotifywait : 在被监控的文件或目录上等待特定文件系统事件（open close delete等）发生,执行后处于阻塞状态，适合在shell脚本中使用 inotifywatch :收集被监控的文件系统使用的统计数据,指文件系统事件发生的次数统计。 第三个里程碑：编写脚本，实现rsync+inotify软件功能结合 rsync服务命令 1rsync -avz --delete /data/ rsync_backup@172.16.1.41::nfsbackup --password-file=/etc/rsync.password inotify服务命令： 1inotifywait -mrq /data -format &quot;%w%f&quot; -e create,delete,move_to,close_write 编写脚本 12345678[root@nfs01 sersync]# vim /server/scripts/inotify.sh#!/bin/bashinotifywait -mrq /data --format &quot;%w%f&quot; -e create,delete,moved_to,close_write|\while read linedo rsync -az --delete /data/ rsync_backup@172.16.1.41::nfsbackup --password-file=/etc/rsync.passworddone 脚本说明： for循环会定义一个条件，当条件不满足时停止循环 while循环：只要条件满足就一直循环下去 对脚本进行优化 12345678910111213141516#!/bin/bashPath=/databackup_Server=172.16.1.41/usr/bin/inotifywait -mrq --format &apos;%w%f&apos; -e create,close_write,delete /data | while read line do if [ -f $line ];then rsync -az $line --delete rsync_backup@$backup_Server::nfsbackup --password-file=/etc/rsync.password else cd $Path &amp;&amp;\ rsync -az ./ --delete rsync_backup@$backup_Server::nfsbackup --password-file=/etc/rsync.password fidone 第四个里程碑：测试编写的脚本让脚本在后台运行 在/data 目录先创建6个文件 123[root@nfs01 data]# sh /server/scripts/inotify.sh &amp;[root@nfs01 data]# touch &#123;1..6&#125;.txt 在backup服务器上，已经时候同步过去了6个文件。 123456789101112131415root@backup ~]# ll /nfsbackup/total 8-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 1.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 2.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 3.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 4.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 5.txt-rw-r--r-- 1 rsync rsync 0 Oct 17 12:06 6.txt 利用while循环语句编写的脚本停止方法（kill）ctrl+z暂停程序运行，kill -9杀死 不要暂停程序，直接利用杀手三剑客进行杀进程 说明： kill三个杀手不是万能的，在进程暂停时，无法杀死；kill -9 （危险） 查看后台都要哪些程序在运行123[root@nfs01 data]# jobs[1]+ Running sh /server/scripts/inotify.sh &amp; fg将后台的程序调到前台来123[root@nfs01 data]# fg 1sh /server/scripts/inotify.sh 进程的前台和后台运行方法：123fg -- 前台bg -- 后台 脚本后台运行方法1234501. sh inotify.sh &amp;02. nohup sh inotify.sh &amp;03. screen实现脚本程序后台运行 1sh /server/scripts/inotify.sh &amp; nohup1nohup sh inotify.sh &amp; 原文引自这里]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile详解]]></title>
    <url>%2F2017%2F12%2F16%2FDockerfile%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[基本结构Dockerfile 由一行行命令语句组成，并且支持以# 开头的注释行。 一般的，Dockerfile 分为四部分： 基础镜像信息 维护者信息 镜像操作指令 容器启动时执行指令。 例如 1234567891011121314# This dockerfile uses the ubuntu image# VERSION 2 - EDITION 1# Author: docker_user# Command format: Instruction [arguments / command] ..# Base image to use, this must be set as the first lineFROM ubuntu# Maintainer: docker_user &lt;docker_user at email.com&gt; (@docker_user)MAINTAINER docker_user docker_user@email.com# Commands to update the imageRUN echo &quot;deb http://archive.ubuntu.com/ubuntu/ raring main universe&quot; &gt;&gt; /etc/apt/sources.listRUN apt-get update &amp;&amp; apt-get install -y nginxRUN echo &quot;\ndaemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf# Commands when creating a new containerCMD /usr/sbin/nginx 其中，一开始必须指明所基于的镜像名称，接下来推荐说明维护者信息。 后面则是镜像操作指令，例如RUN 指令， RUN 指令将对镜像执行跟随的命令。每运行一条RUN 指令，镜像添加新的一层，并提交。 最后是CMD 指令，来指定运行容器时的操作命令。 下面是一个更复杂的例子 12345678910111213141516171819202122232425262728293031# Nginx## VERSION 0.0.1FROM ubuntuMAINTAINER Victor Vieux &lt;victor@docker.com&gt;RUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server# Firefox over VNC## VERSION 0.3FROM ubuntu# Install vnc, xvfb in order to create a &apos;fake&apos; display and firefoxRUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefoxRUN mkdir /.vnc# Setup a passwordRUN x11vnc -storepasswd 1234 ~/.vnc/passwd# Autostart firefox (might not be the best way, but it does the trick)RUN bash -c &apos;echo &quot;firefox&quot; &gt;&gt; /.bashrc&apos;EXPOSE 5900CMD [&quot;x11vnc&quot;, &quot;-forever&quot;, &quot;-usepw&quot;, &quot;-create&quot;]# Multiple images example## VERSION 0.1FROM ubuntuRUN echo foo &gt; bar# Will output something like ===&gt; 907ad6c2736fFROM ubuntuRUN echo moo &gt; oink# Will output something like ===&gt; 695d7793cbe4# You᾿ll now have two images, 907ad6c2736f with /bar, and 695d7793cbe4 with# /oink. 指令指令的一般格式为 INSTRUCTION arguments ，指令包括FROM 、MAINTAINER 、RUN 等。 FROM 格式为FROM 或 FROM : 。 第一条指令必须为FROM 指令。并且，如果在同一个Dockerfile中创建多个镜像时，可以使用多个FROM 指令（每个镜像一次）。 MAINTAINER 格式为 MAINTAINER ，指定维护者信息。 RUN 格式为 RUN 或 RUN [“executable”, “param1”, “param2”] 。 前者将在 shell 终端中运行命令，即/bin/sh -c ；后者则使用exec 执行。指定使用其它终端可以通过第二种方式实现，例如RUN [“/bin/bash”, “-c”, “echo hello”] 。 每条RUN 指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用\ 来换行。 CMD 支持三种格式: CMD [“executable”,”param1”,”param2”] 使用exec 执行，推荐方式； CMD command param1 param2 在/bin/sh 中执行，提供给需要交互的应用； CMD [“param1”,”param2”] 提供给ENTRYPOINT 的默认参数； 指定启动容器时执行的命令，每个 Dockerfile 只能有一条CMD 命令。如果指定了多条命令，只有最后一条会被执行。 EXPOSE 格式为EXPOSE […]告诉 Docker 服务端容器暴露的端口号，供互联系统使用。在启动容器时需要通过 -P，Docker 主机会自动分配一个端口转发到指定的端口。 ENV 格式为ENV 。 指定一个环境变量，会被后续RUN 指令使用，并在容器运行时保持。 例如 1234ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH ADD 格式为ADD 该命令将复制指定的 到容器中的 。 其中 可以是Dockerfile所在目录的一个相对路径；也可以是一个 URL；还可以是一个 tar 文件（自动解压为目录）。 COPY 格式为COPY 。 复制本地主机的 （为 Dockerfile 所在目录的相对路径）到容器中的 。 当使用本地目录为源目录时，推荐使用 COPY 。 ENTRYPOINT 两种格式： ENTRYPOINT [“executable”, “param1”, “param2”] ENTRYPOINT command param1 param2 （shell中执行）。 配置容器启动后执行的命令，并且不可被docker run 提供的参数覆盖。每个 Dockerfile 中只能有一个ENTRYPOINT ，当指定多个时，只有最后一个起效。 VOLUME 格式为VOLUME [“/data”] 。创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。 USER 格式为USER daemon 。指定运行容器时的用户名或 UID，后续的RUN 也会使用指定用户。当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如： RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 。要临时获取管理员权限可以使用gosu ，而不推荐sudo 。 WORKDIR 格式为WORKDIR /path/to/workdir 。为后续的RUN 、CMD 、ENTRYPOINT 指令配置工作目录。可以使用多个WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如 1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 则最终路径为/a/b/c 。 ONBUILD 格式为ONBUILD [INSTRUCTION] 。配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。例如，Dockerfile 使用如下的内容创建了镜像image-A 。 1234[...]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build --dir /app/src[...] 如果基于 image-A 创建新的镜像时，新的Dockerfile中使用FROM image-A 指定基础镜像时，会自动执行ONBUILD 指令内容，等价于在后面添加了两条指令。 1234FROM image-A#Automatically run the followingADD . /app/srcRUN /usr/local/bin/python-build --dir /app/src 使用ONBUILD 指令的镜像，推荐在标签中注明，例如ruby:1.9-onbuild 。 创建镜像编写完成 Dockerfile 之后，可以通过docker build 命令来创建镜像。 基本的格式为 ==docker build== [选项] 路径，该命令将读取指定路径下（包括子目录）的 Dockerfile，并将该路径下所有内容发送给 Docker 服务端，由服务端来创建镜像。因此一般建议放置 Dockerfile 的目录为空目录。也可以通过.dockerignore 文件（每一行添加一条匹配模式）来让 Docker 忽略路径下的目录和文件。要指定镜像的标签信息，可以通过-t 选项， 例如 1$ sudo docker build -t myrepo/myapp /tmp/test1/]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu14.04更换内核以安装锐速]]></title>
    <url>%2F2017%2F12%2F16%2FUbuntu14-04%E6%9B%B4%E6%8D%A2%E5%86%85%E6%A0%B8%E4%BB%A5%E5%AE%89%E8%A3%85%E9%94%90%E9%80%9F%2F</url>
    <content type="text"><![CDATA[首先，将现有vps系统更新过最新版本。 123apt-get updateapt-get upgrade 命令 1uname -a 检查当前Kernel版本。结果举例： 1Linux localhost 4.0.4-x86_64-linode57 #1 SMP Thu May 21 11:01:47 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux 接下来是安装grub2组件： 123456789Ubuntu:apt-get update apt-get install linux-image-virtual grub2Debian:apt-get install linux-image-amd64 grub2 注意！远程窗口会显示Grub安装界面，要求你选择grub安装位置，你直接选择不安装即可。 查看默认系统内核版本： ls /boot/vmlinuz* 接下来我们配置grub启动参数： 1vi /etc/default/grub 将grub配置文件修改以下参数： 12345GRUB_TIMEOUT=10GRUB_CMDLINE_LINUX=&quot;console=ttyS0,19200n8&quot;GRUB_DISABLE_LINUX_UUID=trueGRUB_SERIAL_COMMAND=&quot;serial -speed=19200 -unit=0 -word=8 -parity=no -stop=1&quot;GRUB_TERMINAL=serial 更新bootloader： Debian 8 &amp; Ubuntu 15.04： 更新系统引导文件 1update-grub 点击Linode后台面板Dashboard，点击Edit按钮： 在Kernel下拉菜单，选择GRUB 2启动： 重启vps后，再次输入uname -a可显示内核版本号。 举例：Linux li63-119.members.linode.com 3.10.0-229.4.2.el7.x86_64 至此，你已摆脱了Linode官方默认的内核，可随意安装任意版本号的Kernel，然后修改grub菜单指定启动选项。 Linode官网 www.linode.com 如果是用Linode竞争对手的产品，无论是Vultr和Digitalocean都可轻松更换内核，比linode方便很多，而且默认直接支持hybla阻塞算法，有良好的加速效果。 参考文档 https://since1989.org/linode/centos-ubuntu-kernel-linux-grub2.html 第二步 破解版锐速最新更新 =======2016年8月7日更新===========： 新增了以下支持的内核，欢迎大家测试，有问题及时反馈： 12345CentOS-6.8：2.6.32-642.el7.x86_64CentOS-7.2：3.10.0-327.el7.x86_64CentOS：4.4.0-x86_64-linode63Ubuntu_14.04：4.2.0-35-genericDebian_8：3.16.0-4-amd64 首先保证你的服务器或VPS是64位系统，锐速不支持任何ubuntu 14.04的32位系统 安装linux-image-4.2.0-35-generic内核文件： 1~# apt-get install linux-image-4.2.0-35-generic 复制代码 查看当前安装的内核： 1~# dpkg -l|grep linux-image 复制代码 这里会返回刚才装的linux-image-4.2.0-35-generic内核和之前服务器上安装的内核，我们要做的就是卸载以前安装的内核. 卸载第3步中看到的其他内核： 1~# apt-get purge linux-image-3.13.0-110-generic linux-image-3.13.0-95-generic 复制代码 这里的xx是第3步中看到的当前服务器或VPS上安装的其他内核，注意如果当前服务器安装的不是最新的内核，卸载的同时会给服务器安装最新内核；为了能让服务器使用锐速支持的3.13.0-24-generic内核，我们还要再执行一次这个命令，把安装的最新内核卸载掉。 更新grub系统引导文件： #~ sudo update-grub复制代码 重启系统： #~ sudo reboot复制代码 重启之后使用 #~ uname -r复制代码就可以看到服务器已经使用锐速支持的3.13.0-24-generic内核了，这时候就可以去安装锐速了 参考文档 第三步安装锐速 (一键安装) 锐速破解版安装方法： wget -N –no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh &amp;&amp; bash serverspeeder-all.sh vi /serverspeeder/etc/config 锐速破解版卸载方法： chattr -i /serverspeeder/etc/apx* &amp;&amp; /serverspeeder/bin/serverSpeeder.sh uninstall -f 查看锐速启动状态/serverspeeder/bin/serverSpeeder.sh status 第二种方式 锐速手动安装 1wget -N --no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/test/serverspeeder-v.sh &amp;&amp; bash serverspeeder-v.sh Ubuntu 14.04 4.2.0-35-generic x64 3.11.20.4 serverspeeder_42035 第三种安装方式 锐速 (lotServer) 一键安装新增 lotServer 安装，建议首先尝试使用，安装失败再尝试其他方式有部分使用者可能存在91yun的锐速破解后存在断流问题，可以尝试锐速 (lotServer)参考 lotServer是锐速的母公司，面向企业用户。大家比较熟悉的锐速其实是lotServer的马甲首先查一下，是否有支持所采用的系统的内核存在看这里安装 参考文档： Ubuntu14.04更换内核以安装锐速]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网精编]]></title>
    <url>%2F2017%2F12%2F16%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E7%B2%BE%E7%BC%96%2F</url>
    <content type="text"><![CDATA[前景 谁比较需要单独购买VPS科学上网？ 只是翻墙简单浏览网页，就不必要再单独花费人民币了，蓝灯、和各个网站的免费ss账户，是你的主菜。 偶尔看看视频、不想画太多精力在寻找免费资源上。可以直接购买现成的ss/vpn账户，在G+相应的社区可以找到很多比较靠谱的提供商。 常年泡视频网站又受不了国内视频网站乱七八糟、24h使用google服务、对稳定性和流量有一定要求、愿意使用一点时间来折腾VPS。 搜索文档，更新软件源、折腾个人网站又不想备案的、折腾开源的 这部分大神谁会来看这个啊，早自己分分钟写个轮子了 注意，本教程 不是 图文并茂的面向小白的教程，窝希望你能够有足够的 Linux 操作经验再来看这篇教程。至少你需要熟悉 ssh 连接，熟悉 Web 环境的配置，最好可以看得懂一些代码。 前端搭建搭建环境 Linux ubuntu 4.2.0-35-generic #40~14.04.1-Ubuntu x86_64 GNU/Linuxmysql Ver 14.14 Distrib 5.5.48 (Enable InnoDB: y)nginx-1.10.0mysql-5.5.48php-7.0.7安装lnmp 1.3 步骤开始： [x] linode更换Linux内核通过锐速加速 请参看这篇博文 整合ss-panel目前主要是支持了 VPN 自动开户，自动销毁，流量计入面板的总流量，以及弄了个公告系统，还有和 DirectAdmin 对接，还有可以支持 PAC 等方式的接入。 添加一个虚拟主机，同时创建数据库。 1lnmp vhost add 配置好nginx运行环境之后，就是下载mod ss-panel 123456cd /home/wwwroot/ss.panelyum install git -ygit clone https://github.com/glzjin/ss-panel-v3-mod.git tmp -b new_master &amp;&amp; mv tmp/.git . &amp;&amp; rm -rf tmp &amp;&amp; git reset --hardchown -R root:root *chmod -R 777 *chown -R www:www storage 修改完了之后，到网站目录下进行一些修改 1234[root@vultr vhost]# cd /home/wwwroot/ss.panel/[root@vultr ss.panel]# chattr -i .user.ini[root@vultr ss.panel]# mv .user.ini public[root@vultr ss.panel]# cd public 然后就是重新添加回权限。 1chattr +i .user.ini OK，重启一下 nginx 1service nginx restart 编辑完后重载你的 Web 服务器，然后访问你的站点……于是你得到了一个 500 Internal Server Error（如果你没开启 display_errors 可能看不到详细报错）： 1234Warning: require(/home/wwwroot/ss.prinzeugen.net/vendor/autoload.php): failed to open stream: No such file or directory in /home/wwwroot/ss.prinzeugen.net/bootstrap.php on line 18Fatal error: require(): Failed opening required &apos;/home/wwwroot/ss.prinzeugen.net/vendor/autoload.php&apos; (include_path=&apos;.:/usr/local/php/lib/php&apos;) in /home/wwwroot/ss.prinzeugen.net/bootstrap.php on line 18 这是我们还未安装 ss-panel 所需的依赖库导致的。遂安装之： 12$ curl -sS https://getcomposer.org/installer | php$ php composer.phar install 等待它安装完毕后接着进行配置： 1cp .env.example .env 将 .env.example 复制一份重命名为 .env，自行修改其中的数据库等信息。 12345678910# database 数据库配置db_driver = &apos;mysql&apos;db_host = &apos;localhost&apos;db_port = &apos;3306&apos;db_database = &apos;ss-panel&apos;db_username = &apos;ss-panel&apos;db_password = &apos;secret&apos;db_charset = &apos;utf8&apos;db_collation = &apos;utf8_general_ci&apos;db_prefix = &apos;&apos; 数据库的创建我就不多说了，建站的一般都玩过数据库吧？将根目录下的 db.sql 导入到数据库中即可。其他配置自行修改。 你还需要修改 .env 中的 muKey 字段，修改为任意字符串（最好只包含 ASCII 字符），下面配置后端的时候我们需要使用到这个 muKey： 1muKey = &apos;api_key_just_for_test&apos; 接下来，确保 storage 目录可写入（否则 Smarty 会报错）： 1$ chown -R www storage 现在访问你的站点，就可以看到 ss-panel 的首页啦： 1$ php xcat createAdmin Auth Driver 认证设置ss-panel v3 配置说明，请根据说明合理选择密码加密方式，认证方式等 ss-panel v3支持多种存储用户认证信息的方式： file 使用文件存储sessions。 redis 使用Redis存储，推荐此方式。 推荐使用redis 计划任务接下来需要对服务器进行计划任务的设置, 执行 crontab -e命令, 添加以下五段 123456730 22 * * * php /home/wwwroot/站点文件夹/xcat sendDiaryMail */1 * * * * php /home/wwwroot/站点文件夹/xcat synclogin*/1 * * * * php /home/wwwroot/站点文件夹/xcat syncvpn0 0 * * * php -n /home/wwwroot/站点文件夹/xcat dailyjob*/1 * * * * php /home/wwwroot/站点文件夹/xcat checkjob */1 * * * * php -n /home/wwwroot/站点文件夹/xcat syncnas 重启Crontab /etc/init.d/cron restart 查看防火墙规则1234查看已添加的iptables规则iptables -L -n --line-numbers删除已添加的iptables规则iptables -D INPUT line-numbers 规则保存1234# Ubuntuiptables-save &gt; /etc/iptables.rules# CentOSservice iptables save 部署并配置 shadowsocks-manyuser(后端)连接上 shadowsocks 试试看能不能翻墙了？八成不能。 虽然你成功的把 servers.py 跑起来了，但还可能有各种神奇的错误阻止你翻出伟大的墙。 首先国际惯例查看连接： 1$ netstat -anp | grep 你的端口 正常的话，应该是这样的： 如果没有来自你的 IP 的 TCP 连接的话，那八成就是防火墙的锅了，执行 iptables 放行你的端口： 12$ iptables -I INPUT -p tcp -m tcp --dport 你的端口 -j ACCEPT$ iptables-save 1python server.py 这句运行代码主要用于调试，关闭ssh后ss后端自动关闭，所以正式使用请使用下面的脚本运行！如果需要停止请按Ctrl+C键终止程序。这时可查看有运行情况，检查有没有错误。如果服务端没有错误，而连接不上，需要检查iptables或firewall(ubuntu)的防火墙配置 ss-panel 新注册的用户所分配的端口均为其 id-1 的用户的端口号 + 1。比如说你把 admin 用户（uid 为1）的端口改为 12450（ss-panel 中不能改，去数据库改），那么后面注册的新用户的端口就会是 12451, 12452 这样递增的。 所以如果你要开放注册，就要这样配置你的 iptables： 123# 注意是半角冒号，意为允许 12450 及以上的端口# 也可以指定 12450:15550 这样的范围$ iptables -I INPUT -p tcp -m tcp --dport 12450: -j ACCEPT 现在再连接 shadowsocks 就应该可以看到 TCP 连接信息了。并且你可以在 ss-mu 后端的输出信息中看到详细的连接日志： 使用supervisor监控ss-manyuser后端运行安装 supervisor （用的是上面安装过的 pip）： 12345# 先安装 pip 包管理器$ sudo apt-get install python-pip # For Debian/Ubuntu$ sudo yum install python-pip # For CentOS$ pip install supervisor 创建 supervisor 配置文件 12# 输出至 supervisor 的默认配置路径$ echo_supervisord_conf &gt; /etc/supervisord.conf 运行 supervisor 服务 1$ supervisord 配置 supervisor 以监控 ss-manyuser 运行 1$ vim /etc/supervisord.conf 在文件尾部（当然也可以新建配置文件，不过这样比较方便）添加如下内容并酌情修改： 12345[program:ss-manyuser]command = python /root/shadowsocks-py-mu/shadowsocks/servers.pyuser = rootautostart = trueautorestart = true 其中 command 里的目录请自行修改为你的 servers.py 所在的绝对路径。 重启 supervisor 服务以加载配置 1$ killall -HUP supervisord 查看 shadowsocks-manyuser 是否已经运行： 12$ ps -ef | grep servers.pyroot 952 739 0 15:40 ? 00:00:00 python /root/shadowsocks-rm/shadowsocks/servers.py 可以通过以下命令管理 shadowsock-manyuser 的状态 1$ supervisorctl &#123;start|stop|restart&#125; ss-manyuser ss-panel 的多节点配置其实多节点也没咋玄乎，说白了就是多个后端共用一个前端而已。而且我们的后端是使用 Mu API 来与前端进行交互的，所以多节点的配置就更简单了：只要把所有后端的 config.py 中的 API_URL 和 API_PASS 都改成一样即可（记得 API_ENABLED = True）。 Linux chattr命令Linux chattr命令用于改变文件属性。这项指令可改变存放在ext2文件系统上的文件或目录属性，这些属性共有以下8种模式： 用chattr命令防止系统中某个关键文件被修改： 1chattr +i /etc/resolv.conf 1lsattr /etc/resolv.conf 会显示如下属性 1----i-------- /etc/resolv.conf 让某个文件只能往里面追加数据，但不能删除，适用于各种日志文件： Step 4 123456789Nginx Config example:if you download ss-panel on path /home/www/ss-panelroot /home/www/ss-panel/public;location / &#123; try_files $uri $uri/ /index.php$is_args$args;&#125; 1chattr +a /var/log/messages 参考文档在这里 报错： 1234&gt; PHP Warning: require(/home/wwwroot/mod-ss-panel/vendor/autoload.php): failed to open stream: No such file or directory in /home/wwwroot/mod-ss-panel/bootstrap.php on line 17PHP Fatal error: require(): Failed opening required &apos;/home/wwwroot/mod-ss-panel/vendor/autoload.php&apos; (include_path=&apos;.:/usr/local/php/lib/php&apos;) in /home/wwwroot/mod-ss-panel/bootstrap.php on line 17&gt; Failed opening required &apos;vendor/autoload.php&apos; after update 解决方法 1Try to run &quot;composer install&quot; in the project root, since the master branch does not automatically include all dependencies (opposing to stable releases). 原文在这里 报错 1ss-panel-v3-mod No input file specified 解决方法 更改php.ini首先php.ini的配置中把;cgi.fix_pathinfo=0 改为cgi.fix_pathinfo=1 原文在这里 mod版官方wiki 报错 1ImportError: No module named cymysql 解决方法 安装cymysql 1pip install cymysql 原文在这里 报错 1The server quit without updating PID file (/var/run/mysqld/mysqld.pid). 解决方法 /usr/local/var/目录下面也确实生成了mysql目录，但是还是无法启动MySql，每个目录该给的权限都给了，所有者以及所有组都改成mysql了，但是还是不行，网上各种方法也都试了，最后无果，就在CSDN上面提出了问题，有个热心网友回答了我的问题，他给了一个链接，其实那个链接的文章我看过了，但是看到删除my.cnf的时候我突然想到/etc/mysql/下面也有个my.cnf文件，于是删除了那个文件，启动MySql服务，成功了！ 原文在这里 参考文档： 可能是最好的ss-panel部署教程 安装ss-go-mu版本和ss-panel v3 ShadowsocksR多用户版服务端安装教程 shadowsocks-py-mu 搭建 sspanel v3 魔改版记录]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keepalived + 反向代理 + http服务构建实用性Web站点]]></title>
    <url>%2F2017%2F12%2F13%2Fkeepalived-%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86-http%E6%9C%8D%E5%8A%A1%E6%9E%84%E5%BB%BA%E5%AE%9E%E7%94%A8%E6%80%A7Web%E7%AB%99%E7%82%B9%2F</url>
    <content type="text"><![CDATA[反向代理又有什么用呢？既然有反向代理，那就肯定有正向代理，先说下正向代理。在我们上网的时候，有时候我们不是直接访问某个网站，而是通过某个代理来访问某个网站，然后，再由该代理将结果返回给我们。 这里，这个代理就是正向代理。也就是说在客户端使用的代理叫做正向代理。而反向代理正好相反。 在服务器端，有些服务器出于安全及负载等考虑，不是让客户端直接访问，而是在服务器端，设置一个代理，将所有用户的请求都先发至该代理，然后，通过代理向服务器端请求相应的资源，然后由该代理向客户端回应。这里的代理就是反向代理。 在网络中正向代理、反向代理如下图所示： 本文主要介绍如何结合keepalived、反向代理和http服务三者一块工作，共同构建一个高效的、实用性的web服务站点。 通过一个图，来介绍下如何构建此架构，以及列举出一些常用的提供反向代理和http服务的技术都有哪些。 从上图可以看到，这里，我将keepalived和反向代理放在同一个主机上实现，后端是提供http服务的主机，采用类似于LVS的NAT模型来实现此架构。 本博客，仅介绍和演示两种提供反向代理及http服务的技术，分别为：nginx、haproxy和apache、nginx。 在前面的博客中，我已经介绍过，keepalived主要用来实现对我们的路由实现高可用，以免当只有一个路由时，因该路由故障导致内网无法与外部通信的问题。这里相关的理论知识就不在介绍了。在LNMP相关博文中，我已经介绍过，nginx除了可以提供http服务之外，还有一个重要的功能，那就是可用来做反向代理，这里，正是将nginx作为反向代理来用，同时也结合nginx提供的http服务一起工作。nginx由于其低资源占用，在处理客户端请求时，一万个并发请求也仅占用2.5M内存空间，因此，可以想象为何众多网站采用nginx做代理来实现了吧。haproxy是专用来提供反向代理的服务软件。 待会在细细介绍haproxy。在LAMP中，已经介绍过apache了，因此，这里也不再过多介绍，仅演示实验。我会在后面的博客中专门介绍apache做代理，后端采用tomcat提供http服务的相关博客，敬请关注。 先介绍下haproxy什么是haproxy呢？先来看下百度百科是如何介绍haproxy的吧。 HAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案。HAProxy特别适用于那些负载特大的web站点，这些站点通常又需要会话保持或七层处理。HAProxy运行在时下的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。 HAProxy实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。多进程或多线程模型受内存限制 、系统调度器限制以及无处不在的锁限制，很少能处理数千并发连接。事件驱动模型因为在有更好的资源和时间管理的用户端(User-Space) 实现所有这些任务，所以没有这些问题。此模型的弊端是，在多核系统上，这些程序通常扩展性较差。这就是为什么他们必须进行优化以 使每个CPU时间片(Cycle)做更多的工作。 HAProxy是免费、极速且可靠的用于为TCP和基于HTTP应用程序提供高可用、负载均衡和代理服务的解决方案，尤其适用于高负载且需要持久连接或7层处理机制的web站点。 演示实验包括以下两个： 12一、keepalived + nginx反向代理 + apache服务构建Web站点二、keepalived + haproxy反向代理 + apache服务构建Web站点 此处，keepalived用来提供虚拟路由的功能，采用两台主机来实现此功能；nginx反向代理功能与keepalived一起在同一主机上实现。在提供反向代理功能方面，还有另外一个也可以实现反向代理的功能，那就是haproxy，关于haproxy的反向代理功能的实现，会在后面的博客中介绍，敬请关注，这里主要介绍nginx的反向代理功能的实现。 apache用来提供http服务，后端也可以是一个基于LAMP架构的服务器。在上一遍博客中，提供的一张图片显示，apache的市场份额依然占据着领导地位，其强大的市场份额一时半会还没有哪个能够超越，因此，在企业中，基于LAMP架构来提供http服务还是很多的。这里我们就不演示基于LAMP架构来实现此三者的组合了，有兴趣的话读者可参考我的另一篇博客中介绍的方法自行构建一个LAMP，然后加上nginx的反向代理功能，再加上keepalived来提供虚拟路由功能，将三者结合起来一起工作。 实验环境： 本次所示实验都在虚拟机上实现。 123456789系统：RHEL5.8；内核：linux-2.6.18-308.el5；基于keepalived的主从模型实现，后端采用类似于LVS的NAT模型实现。各IP分配如下所示： VIP: 172.16.32.5 DIP: 172.16.32.30 对应主机名为：node1 172.16.32.31 对应主机名为：node2 RIP: 172.16.32.32 对应主机名为：node3 172.16.32.33 对应主机名为：node4 实现集群的前提： 1231、时间同步2、双机互信3、主机名解析 以上三个前提在前面的博客中我已经介绍过如何实现了，这里就不在介绍了，不知道的请参考我以前的博客。现在开始演示我们的实验。 一、keepalived + nginx反向代理 + apache提供的http服务构建Web站点 先看下实现模型图： 在上图中，keepalived和nginx反向代理在同一个主机上实现，后端是apache提供的http服务。 首先，安装好yum源，然后先去提供http服务的两台后端主机node3和node4上安装apache，以提供http服务。 以下操作需在node3和node4上都进行： 123456rpm -q httpd #查看是否安装此软件包，如果没有配置好yum源后安装此软件包yum -y install httpd #安装该软件包vim /var/www/html/index.html #编辑该文件，添加如下两行信息，以提供主页&lt;h1&gt;http://lq2419.blog.51cto.com/&lt;/h1&gt;&lt;h2&gt;Apache node3, IP: 172.16.32.32&lt;/h2&gt; #在node4主机上，修改&lt;h2&gt;Apache node4, IP: 172.16.32.33&lt;/h2&gt;service httpd start #启动httpd服务 在我们的物理机上进行测试，显示效果如下所示： 下面是实现的日志，这里进贴出其中一个主机的日志。 1234567891011tail /var/log/httpd/access_log #在node3查看日志信息，查看显示访问的IP，因为是通过我们的物理机直接访问，所以显示的是物理机IP172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:46 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET / HTTP/1.1&quot; 200 79 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot;172.16.32.0 - - [25/May/2013:20:55:47 +0800] &quot;GET /favicon.ico HTTP/1.1&quot; 404 287 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31&quot; 修改httpd的配置文件，对日志显示格式进行修改。 123vim /etc/httpd/conf/httpd.conf #修改该文件LogFormat &quot;%&#123;X-Real-IP&#125;i %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%&#123;Referer&#125;i\&quot; \&quot;%&#123;User-Agent&#125;i\&quot;&quot; combined #找到该项，将第一个%h修改%&#123;X-Real-IP&#125;i，用于当使用代理访问时显示客户端IP，而不是代理IPservice httpd restart #重启httpd服务 以上修改需在两台后端服务器主机上都进行。为了看效果，你也可以先不修改后端服务器httpd配置文件中日志格式的显示情况，先启动代理服务，使用物理机访问代理主机，然后在服务器主机上查看访问日志，看显示的客户端IP是否正常，是代理IP还是物理机IP。然后再来修改此选项。这里不再演示。 后端的两台http服务器已配置完毕。接着去前端的两台代理主机node1和node2上进行相关的配置。 以下操作在需在node1和node2上都执行： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144yum -y install pcre-develtar xf nginx-1.4.1.tar.gzcd nginx-1.4.1./configure \ #执行该命令，各参数相关含义前面博客中已经介绍过，这里就不再介绍了 --prefix=/usr \ --sbin-path=/usr/sbin/nginx \ --conf-path=/etc/nginx/nginx.conf \ --error-log-path=/var/log/nginx/error.log \ --http-log-path=/var/log/nginx/access.log \ --pid-path=/var/run/nginx/nginx.pid \ --lock-path=/var/lock/nginx.lock \ --user=nginx \ --group=nginx \ --with-http_ssl_module \ --with-http_flv_module \ --with-http_stub_status_module \ --with-http_gzip_static_module \ --http-client-body-temp-path=/var/tmp/nginx/client/ \ --http-proxy-temp-path=/var/tmp/nginx/proxy/ \ --http-fastcgi-temp-path=/var/tmp/nginx/fcgi/ \ --http-uwsgi-temp-path=/var/tmp/nginx/uwsgi \ --http-scgi-temp-path=/var/tmp/nginx/scgi \ --with-pcre \ --with-file-aio提供SysV风格的服务脚本：vim /etc/init.d/nginx#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15# description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0nginx=&quot;/usr/sbin/nginx&quot;prog=$(basename $nginx)NGINX_CONF_FILE=&quot;/etc/nginx/nginx.conf&quot;[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginxlockfile=/var/lock/subsys/nginxmake_dirs() &#123; # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125;start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125;stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125;restart() &#123; configtest || return $? stop sleep 1 start&#125;reload() &#123; configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125;force_reload() &#123; restart&#125;configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125;rh_status() &#123; status $prog&#125;rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125;case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot; exit 2esac赋予其执行权限：chmod +x /etc/init.d/nginxservice httpd stop #如果http服务启动，需先关闭chkconfig httpd off #关闭开机自启动chkconfig --add nginx #添加到服务列表chkconfig nginx on #开机自启动chkconfig --list nginxservice nginx start #启动服务 到此两台主机上的nginx就算是安装好了，我们只需添加相关代理的配置即可。现在直接配置我们的代理，使其能够工作。 123456789101112131415161718vim /etc/nginx/nginx.conf #编辑此配置文件，添加与代理相关的配置 upstream webserver &#123; #在http段，添加新的上下文 server 172.16.32.32 weight=1 max_fails=2 fail_timeout=2; #定义后端服务器，权重为1，最大尝试失败次数为2，失败时两次尝试的超时时长为2 server 172.16.32.33 weight=1 max_fails=2 fail_timeout=2; server 127.0.0.1 backup; #定义当上边两个都down机后，启用本机的服务，只要有一个还提供服务，就不启用此主机的服务 &#125; server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; #删除根下所有的默认配置，添加如下两行 proxy_pass http://webserver; #将所有访问根的都转发的webserver proxy_set_header X-Real-IP $remote_addr; #设置转发的头部，此项可实现让后端服务器记录客户端IP，否则记录的是代理的IP，但后端服务器的记录格式也需要相应修改 &#125;同样，将上边配置好的文件发给另一台代理主机：scp /etc/nginx/nginx.conf node2:/etc/nginx #将此配置文件传给另一个代理主机，假如你没有实现主机名解析，请写成相应的IP地址service nginx reload #重新载入nginx服务 知识点补充： upstream模块可定义一个新的上下文，它包含了一组宝岛upstream服务器，这些服务器可能被赋予了不同的权重、不同的类型甚至可以基于维护等原因被标记为down。 1234567891011upstream模块常用的指令有：ip_hash：基于客户端IP地址完成请求的分发，它可以保证来自于同一个客户端的请求始终被转发至同一个upstream服务器；keepalive：每个worker进程为发送到upstream服务器的连接所缓存的个数；least_conn：最少连接调度算法；server：定义一个upstream服务器的地址，还可包括一系列可选参数，如： weight：权重； max_fails：最大失败连接次数，失败连接的超时时长由fail_timeout指定； fail_timeout：等待请求的目标服务器发送响应的时长； backup：用于fallback的目的，所有服务均故障时才启动此服务器； down：手动标记其不再处理任何请求；upstream模块的负载均衡算法主要有三种，轮调(round-robin)、ip哈希(ip_hash)和最少连接(least_conn)三种。默认为轮调。 现在在我们的物理机上在测试下，这次我们在IE浏览器上测试，因为谷歌浏览器有缓存，刷新时看不出来效果。进行测试，看我们的代理能否工作，显示效果如下所示： 同样，启动另一个代理主机的nginx服务，看能否实现代理功能，这里不再演示。 最后去安装keepalived。keepalived我们采用安装rpm包的方式，不在使用源码包，首先，请下载keepalived的相关rpm包，这里使用的是keepalived-1.2.7-5.el5.i386.rpm。yum -y –nogpgcheck localinstall keepalived-1.2.7-5.el5.i386.rpm 因前面已经介绍过keepalived配置文件各参数含义，这里不再介绍，直接配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108vim /etc/keepalived/keepalived.conf #修改其配置文件global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_script chk_nginx &#123; #检查nginx服务是否存在 script &quot;killall -0 nginx&quot; interval 2 weight -2 fall 2 rise 1 &#125;vrrp_script chk_schedown &#123; #用于手动控制keepalived的主从模型 script &quot;[[ -f /etc/keepalived/down ]] &amp;&amp; exit 1 || exit 0&quot; interval 2 weight -2&#125;vrrp_instance VI_1 &#123; state MASTER #设置其为master，在node2主机上设置该项为backup interface eth0 virtual_router_id 232 #设置虚拟路由组ID priority 101 #设置优先级，在node2主机上设置该项为100，因为是backup advert_int 1 authentication &#123; auth_type PASS auth_pass langdu &#125; virtual_ipaddress &#123; 172.16.32.5/16 dev eth0 label eth0:0 &#125; track_script &#123; chk_nginx chk_schedown &#125; notify_master &quot;/etc/keepalived/notify.sh master&quot; #根据检查结果不同，向同一脚本传递不同的参数 notify_backup &quot;/etc/keepalived/notify.sh backup&quot; notify_fault &quot;/etc/keepalived/notify.sh fault&quot;&#125;健康检查脚本如下所示：vim /etc/keepalived/notify.sh #健康检查脚本，在node2主机上也添加次脚本#!/bin/bash# Author: MageEdu &lt;linuxedu@foxmail.com&gt;# description: An example of notify script#vip=172.16.32.5contact=&apos;root@localhost&apos;Notify() &#123; mailsubject=&quot;`hostname` to be $1: $vip floating&quot; mailbody=&quot;`date &apos;+%F %H:%M:%S&apos;`: vrrp transition, `hostname` changed to be $1&quot; echo $mailbody | mail -s &quot;$mailsubject&quot; $contact&#125;case &quot;$1&quot; in master) notify master /etc/rc.d/init.d/haproxy start exit 0 ;; backup) notify backup /etc/rc.d/init.d/haproxy restart exit 0 ;; fault) notify fault exit 0 ;; *) echo &apos;Usage: `basename $0` &#123;master|backup|fault&#125;&apos; exit 1 ;;esacservice keepalived start #启动我们的keepalived服务先看下主节点上的日志信息tail /var/log/messages #查看node1节点上的日志信息May 25 19:37:03 node1 Keepalived_vrrp[3822]: VRRP_Script(chk_schedown) succeededMay 25 19:37:04 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 25 19:37:05 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 25 19:37:06 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 25 19:37:06 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) setting protocol VIPs.May 25 19:37:06 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5May 25 19:37:06 node1 Keepalived_vrrp[3822]: Netlink reflector reports IP 172.16.32.5 added #添加虚拟IPMay 25 19:37:06 node1 Keepalived_healthcheckers[3821]: Netlink reflector reports IP 172.16.32.5 addedMay 25 19:37:11 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5ifconfig #查看node1上各端口的IP配置eth0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AF inet addr:172.16.32.30 Bcast:172.16.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:655564 errors:7 dropped:0 overruns:0 frame:0 TX packets:66292 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:76104124 (72.5 MiB) TX bytes:9021082 (8.6 MiB) Interrupt:59 Base address:0x2000eth0:0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AF #添加有虚拟IP inet addr:172.16.32.5 Bcast:0.0.0.0 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 Interrupt:59 Base address:0x2000lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:2472 errors:0 dropped:0 overruns:0 frame:0 TX packets:2472 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:227776 (222.4 KiB) TX bytes:227776 (222.4 KiB) 为了查看效果，我们依然在IE浏览器上查看。 我们知道，配置keepalived就是想实现地址漂移的，那IP到底会不会漂移呢，现在我们去试一下。在keepalived的相关配置文件中，提到，只需在/etc/keepalived目录下创建一个down文件即可实现手动漂移。现在我们创建个文件，尝试一下。 在master主机上，这里是node1，请查看你的哪台主机是master，创建一个down文件。 1234567891011121314151617181920212223touch /etc/keepalived/down #创建该文件，用以实现手动实现IP地址漂移tail /var/log/messages #查看node1的日志信息May 25 19:37:06 node1 Keepalived_vrrp[3822]: Netlink reflector reports IP 172.16.32.5 addedMay 25 19:37:06 node1 Keepalived_healthcheckers[3821]: Netlink reflector reports IP 172.16.32.5 addedMay 25 19:37:11 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5May 25 20:41:02 node1 Keepalived_vrrp[3822]: VRRP_Script(chk_schedown) failedMay 25 20:41:03 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Received higher prio advertMay 25 20:41:03 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) Entering BACKUP STATE #进入backup状态May 25 20:41:03 node1 Keepalived_vrrp[3822]: VRRP_Instance(VI_1) removing protocol VIPs.May 25 20:41:03 node1 Keepalived_vrrp[3822]: Netlink reflector reports IP 172.16.32.5 removed #实现IP漂移May 25 20:41:03 node1 Keepalived_healthcheckers[3821]: Netlink reflector reports IP 172.16.32.5 removed现在去查看下node2主机上的日志信息，看是否是master。tail /var/log/messagesMay 25 19:37:15 node1 Keepalived_vrrp[17601]: Netlink reflector reports IP 172.16.32.5 removedMay 25 20:41:15 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 25 20:41:16 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 25 20:41:17 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 25 20:41:17 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) setting protocol VIPs.May 25 20:41:17 node1 Keepalived_healthcheckers[17600]: Netlink reflector reports IP 172.16.32.5 added #添加虚拟IPMay 25 20:41:17 node1 avahi-daemon[3375]: Registering new address record for 172.16.32.5 on eth0.May 25 20:41:17 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5May 25 20:41:17 node1 Keepalived_vrrp[17601]: Netlink reflector reports IP 172.16.32.5 addedMay 25 20:41:22 node1 Keepalived_vrrp[17601]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for 172.16.32.5 实现IP漂移回来就不在演示了，可自己删除创建的文件，然后查看日志信息。从上面两台主机的日志信息，我们已经看到，可以实现IP地址的漂移，并且在物理机上刷新界面显示也正常。 到此，keepalived + nginx反向代理 + apache构建Web站点我们已经实现。 keepalived + haproxy反向代理 + apache提供的http服务构建Web站点 先看构建的模型图： 还是在我们刚才的主机上进行。但是有一点不好的是，rhel5.8上其yum源里没有自带的haproxy安装包，所以，这里我们采用下载源码包，自己编译安装的方式安装haproxy，本人也是第一次源码编译安装，以前是在rhel6.4上采用rpm包安装的，所以，在源码编译安装的过程中，肯定会遇到各种问题，如果有必要我会将遇到的问题一并贴出来，与大家分享，也好解决当你遇到同样问题时不知如何办才好。好了，废话不多说，开始安装我们的haproxy吧。 在node1和node2主机上关机keepalived和nginx服务。 123456789service keepalived stop #关闭keepalived服务service nginx stop #关闭nginx服务chkconfig nginx off #关闭开机自启动然后去下载我们的haproxy，这里采用haproxy-1.4.22版本，下载地址为http://haproxy.1wt.eu/#downtar xf haproxy-1.4.22.tar.gzcd haproxy-1.4.22uname -r #确定自己的内核版本是多少，这里我的内核是linux-2.6.18-308.el5，所以下边使用linux26make TARGET=linux26 PREFIX=/usr/local/ make install PARFIX=/usr/local/ 注：进入该haproxy-1.4.22目录后，你会发现这里并没有我们常见的configure文件，所以不需要执行./configure命令。TARGET用于指定内核版本，PREFIX用于指定安装文件路径，虽然指定了文件路径，但在该目录下并不会生成该文件，此处还有一点需要注意，后边两个选项一定要写成大写，本人试了，小写会报错。 不管你信不信，反正我是信了。要不你试试. 为haproxy提供配置文件和服务脚本。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798mkdir /etc/haproxy #创建该目录，用来存放我们的配置文件cp examples/haproxy.cfg /etc/haproxy/ #复制该文件当我们的配置文件，但是里边好多需要修改提供SysV风格服务脚本：vim /etc/init.d/haproxy #编辑该文件，添加服务脚本#!/bin/sh## haproxy## chkconfig: - 85 15# description: HAProxy is a free, very fast and reliable solution \# offering high availability, load balancing, and \# proxying for TCP and HTTP-based applications# processname: haproxy# config: /etc/haproxy/haproxy.cfg# pidfile: /var/run/haproxy.pid# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0exec=&quot;/usr/local/sbin/haproxy&quot; #在执行完make install之后，会出现此选项，我们可以直接执行haproxy命令prog=$(basename $exec) #说白了，其实就是让取haproxy，不信，待会启动报错时你就会发现了[ -e /etc/sysconfig/$prog ] &amp;&amp; . /etc/sysconfig/$proglockfile=/var/lock/subsys/haproxy #锁文件check() &#123; $exec -c -V -f /etc/$prog/$prog.cfg #知道为何创建刚才的那个haproxy目录了，如果不创建此目录，这里就需要修改了。假如你没有创建刚才的目录，请相应修改服务脚本中的选项&#125;start() &#123; $exec -c -q -f /etc/$prog/$prog.cfg if [ $? -ne 0 ]; then echo &quot;Errors in configuration file, check with $prog check.&quot; return 1 fi echo -n $&quot;Starting $prog: &quot; # start it up here, usually something like &quot;daemon $exec&quot; daemon $exec -D -f /etc/$prog/$prog.cfg -p /var/run/$prog.pid retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125;stop() &#123; echo -n $&quot;Stopping $prog: &quot; # stop it here, often &quot;killproc $prog&quot; killproc $prog retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125;restart() &#123; $exec -c -q -f /etc/$prog/$prog.cfg if [ $? -ne 0 ]; then echo &quot;Errors in configuration file, check with $prog check.&quot; return 1 fi stop start&#125;reload() &#123; $exec -c -q -f /etc/$prog/$prog.cfg if [ $? -ne 0 ]; then echo &quot;Errors in configuration file, check with $prog check.&quot; return 1 fi echo -n $&quot;Reloading $prog: &quot; $exec -D -f /etc/$prog/$prog.cfg -p /var/run/$prog.pid -sf $(cat /var/run/$prog.pid) retval=$? echo return $retval&#125;force_reload() &#123; restart&#125;fdr_status() &#123; status $prog&#125;case &quot;$1&quot; in start|stop|restart|reload) $1 ;; force-reload) force_reload ;; check) check ;; status) fdr_status ;; condrestart|try-restart) [ ! -f $lockfile ] || restart ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|try-restart|reload|force-reload&#125;&quot; exit 2esac 此服务脚本是参考rhel6.4上的haproxy的服务脚本，稍加改动而成的。假如你不想源码编译安装haproxy的话，也可以在rhel6.4上进行此实验。但是rhel6.4上有个地方需要修改，待会再说。 123chmod +x /etc/init.d/haproxychkconfig --add haproxychkconfig haproxy on 先不要启动haproxy服务，先去修改配置文件，将其修改成我们需要的。先介绍下haproxy配置文件中的选项含义 haproxy的配置文件中的选项可分为两个类： 12“global”配置段，用于设定全局配置参数；proxy相关配置段，如“defaults”、“listen”、“frontend”和“backend”； 原文引自这里]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于keepalived(主从+双主) + LVS(DR模型) + DNS实现http高可用集群服务]]></title>
    <url>%2F2017%2F12%2F13%2F%E5%9F%BA%E4%BA%8Ekeepalived-%E4%B8%BB%E4%BB%8E-%E5%8F%8C%E4%B8%BB-LVS-DR%E6%A8%A1%E5%9E%8B-DNS%E5%AE%9E%E7%8E%B0http%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[什么是keepalived呢？keepalived是实现高可用的一种轻量级的技术手段，主要用来防止单点故障(单点故障是指一旦某一点出现故障就会导致整个系统架构的不可用)的发生。之所以说keepalived是轻量级的，是相对于corosync + ldirectord来说的。 keepalived也可以实现高可用集群，而且配置起来比corosync + ldirectord简单方便很多，keepalived与corosync的工作机制相差很多。 corosync + ldirectord实现的功能虽然强大，但配置起来比较麻烦，而keepalived功能虽然简单，但配置起来比较容易。 也就是说keepalived可实现corosync + ldirectord实现的功能，只不过前者没有后者功能强大而已。这里主要介绍keepalived。 在介绍keepalived之前，不得不先介绍下一个协议——VRRP。之所以要介绍这个协议，是因为==VRRP协议是keepalived实现的基础==。 下面先来一块看下这个这协议是干吗用的吧。 如上图所示，通常，同一网段内的所有主机都设置一条相同的、以网关为下一跳的缺省路由。 主机发往其他网段的报文将通过缺省路由发往网关，再由网关进行转发，从而实现主机与外部网络的通信。 当网关发生故障时，本网段内所有以网关为缺省路由的主机将无法与外部网络通信，仅能实现内部主机间通信。 缺省路由为用户的配置操作提供了方便，但是对缺省网关设备提出了很高的稳定性要求。 增加出口网关是提高系统可靠性的常见方法，此时如何在多个出口之间进行选路就成为需要解决的问题。而VRRP正好解决了此问题。 VRRP：Virtual Router Redundancy Protocol，虚拟路由冗余协议。 VRRP说白了就是实现地址漂移的，是一种容错协议，在提高可靠性的同时，简化了主机的配置。 该协议能够实现将可以承担网关功能的一组路由器加入到备份组中，形成一台虚拟路由器，由VRRP的选举机制决定哪台路由器承担转发任务，局域网内的主机只需将虚拟路由器配置为缺省网关。 在VRRP协议出现之前，为了不让单个路由器成为本地与外部通信的瓶颈，我们需要有多个路由，在此种模式下，我们内部的主机就需要将自己的网关指向不同的路由器，这样的配置对我们的网关管理员来说是很麻烦的，且不容易实现。 在VRRP协议出现后，为了不让单个路由器成为本地与外部通信的瓶颈，我们仍需要有多个路由，但可以使用同一个缺省网关，我们只需将内部主机指定一个缺省网关即可。VRRP协议会根据优先级来选择一个正常的路由作为主路由器实现与外部的通信，而其他路由则作为备份路由不参与转发。 在此模式下，多个路由器组成虚拟路由器组，物理上是多个路由器组成，但在逻辑上却是表现为只有一个路由。 效果如下图所示： 在上图中，Router A、Router B和Router C组成一个虚拟路由器。 各虚拟路由器都有自己的IP地址。局域网内的主机将虚拟路由器设置为缺省网关。 Router A、Router B和Router C中优先级最高的路由器作为Master路由器，承担网关的功能。其余两台路由器作为Backup路由器。当master路由器出故障后，backup路由器会根据优先级重新选举新的master路由器承担网关功能。 Master 路由器周期性地发送VRRP 报文，在虚拟路由器中公布其配置信息（优先级等）和工作状况。Backup路由器通过接收到VRRP 报文的情况来判断Master 路由器是否工作正常。 VRRP根据优先级来确定备份组中每台路由器的角色（Master 路由器或Backup 路由器）。 优先级越高，则越有可能成为Master 路由器。VRRP优先级的可配置的取值范围为1 到254。 为了防止非法用户构造报文攻击备份组，VRRP通过在VRRP报文中增加认证字的方式，验证接收到的VRRP报文。 VRRP提供了两种认证方式： simple：简单字符认证。发送VRRP 报文的路由器将认证字填入到VRRP 报文中，而收到VRRP 报文的路由器会将收到的VRRP 报文中的认证字和本地配置的认证字进行比较。如果认证字相同，则认为接收到的报文是真实、合法的VRRP 报文；否则认为接收到的报文是一个非法报文。 md5：MD5 认证。发送VRRP 报文的路由器利用认证字和MD5 算法对VRRP 报文进行摘要运算，运算结果保存在Authentication Header（认证头）中。收到VRRP报文的路由器会利用认证字和MD5 算法进行同样的运算，并将运算结果与认证头的内容进行比较。如果相同，则认为接收到的报文是真实、合法的VRRP 报文；否则认为接收到的报文是一个非法报文。 在有多个路由器组成的虚拟路由中，当我们的内部主机很多时，如果所有主机都使用同一个master路由，会使得其他路由器很清闲，很浪费资源，我们期望我们本地的内部主机平分到各个路由器上，即让我们的内部主机的缺省网关指向不同的路由，从而减轻因只有一个master路由而造成网络带宽拥堵的负担。 这就是负载分担VRRP。但这个如何实现呢？先看下面的配置效果图： 在此情况下，同一台路由器同时加入多个VRRP备份组，在不同备份组中有不同的优先级，从而实现负载分担。 在上图中，有三个备份组存在： 备份组1：对应虚拟路由器1。Router A作为Master路由器，Router B和Router C作为Backup路由器。备份组2：对应虚拟路由器2。Router B作为Master路由器，Router A和Router C作为Backup路由器。备份组3：对应虚拟路由器3。Router C作为Master路由器，Router A和Router B作为Backup路由器。 为了实现业务流量在Router A、Router B和RouterC之间进行负载分担，需要将局域网内的主机的缺省网关分别设置为虚拟路由器1、2和3。在配置优先级时，需要确保三个备份组中各路由器的VRRP优先级形成交叉对应。 为了便于理解，这里给出一张表，我们假定有三个路由设备Router A、B、C和三台主机Host A、B、C，列举有在不同的虚拟路由组中，各路由器所具有的VRRP优先级如下表所示。 从上表可以看到，各路由分别属于不同的虚拟路由组。 对路由器A来说，因在虚拟路由组1中Router A的优先级高于另外两个，因此，Router A 作为 Master 路由器，Router B 和Router C 作为 Backup路由器； 同样，对路由器B来说，因在虚拟路由器组2中Router B的优先级高于另外两个，因此，Router B 作为 Master 路由器，Router A 和Router C 作为 Backup路由器； 对路由器C来说，因在虚拟路由器组3中Router C的优先级高于另外两个，因此，Router C 作为 Master 路由器，Router A 和Router B 作为 Backup路由器。 对不同的主机来说，一旦其master路由器出故障后，会在另外正常的路由器中根据优先级重新选定master路由。 如这里假定Host A的默认网关指向Router A，即Host A指向虚拟路由器组1的默认网关，对主机A来说，如果其master路由出现故障，即Router A出现故障，则会从另外两个正常的备份虚拟路由中根据各自的优先级选取高优先级的作为新的master路由，这里就是选取Router B作为其master路由来完成网关功能。 假如想了解更多关于VRRP协议相关的信息请查阅相关资料，这里不再过多介绍。 在文章开始我就提到，keepalived实现的基础就是基于VRRP协议，上边介绍了那么多关于VRRP协议的相关知识，不知道你是否已经猜到keepalived与VRRP协议到底有什么关系。 keepalived设计之初就是为LVS提供高可用集群的，下面给出一个keepalived官方给的设计图： 从上图可以看出，keepalived包括三个组件：IO复用组件、内存管理组件和控制组件。 在核心组件中IPVS wrapper就是负责生成ipvs规则的，所以说keepalived设计之初就是为ipvs也就是LVS提供高可用集群功能的。 早期的keepalived实现的共能很简单，一般只需三个功能：一个是将IP地址转移到其他节点上，一个就是在另一个主机上生成ipvs规则，最后一个就是健康状况检查。那如何实现IP地址漂移呢，或者说如何实现IP地址转移的呢？这就需要借助于VRRP协议实现了。 keepalived通过软件的方式在其内部模拟实现VRRP协议，然后借助于VRRP协议实现IP地址漂移。现在知道为什么花那么多篇幅介绍VRRP协议了吧。 在下面配置keepalived的时候，还会用到VRRP协议的相关知识，因此一定要清楚的理解VRRP的工作机制，否则在下面配置keepalived的时候你可能会眼花缭乱。 从上边的设计图我们就可以看到，keepalived的出生就是为lvs提供高可用集群服务的，因此，采用keepalived为lvs提供高可用集群服务，配置起来比较简单方便。 在上边我一直强调keepalived设计之初是为lvs提供高可用集群服务的，现在假如我只想通过keepalived为某个服务，不再为lvs高可用服务，比如说通过两台主机为web服务提供高可用服务，该怎么办呢，或者说能否实现呢？ 上边提到，keepalived实现的功能主要有三个，既然我们不为lvs提供高可用服务了，那生成ipvs规则和健康状况检查就不需要了，仅需要提供IP地址转移即可。因此，采用keepalived来实现IP地址转移，我们仍然可以实现为web服务提供高可用服务。 但又遇到另一个问题，我们进实现了IP地址漂移，那我们的web服务怎么办呢，如何实现高可用呢，不能进实现IP地址转移啊，那样的话只能说是IP地址高可用，不能说是web服务高可用？其实现在的keepalived还有其他额外的功能。 keepalived可以通过调用外部脚本的功能，来监控外部其他资源。在keepalived的配置文件中，一般都会有包含如下三行信息： 123vrrp_script &quot;killall -0 SERVICE_NAME&quot;notify-master &quot;script&quot;notify-backup &quot;script&quot; 第一行仅仅实现改变优先级，当IP实现漂移后，在另一个节点上检查是否有该服务，如果有该服务，尝试杀死该服务时会返回正确结果，如果没有该服务会返回错误结果，然后根据返回的结果来修改自身的优先级； 第二行表示，如果是主节点，执行相应的脚本，启动相应的服务； 第三行表示，如果是从节点，执行相应的脚本，停止相应的服务。但是这个脚本需要自己写，这是keepalived中比较麻烦的一个问题。 在下面keepalived的配置文件中会详细介绍如何调用外部脚本来监控其他外部资源。这里不再具体阐述，读者仅仅有个印象即可。枯燥的理论知识终于介绍完毕，你是不是有种解脱的感觉。好吧，下面跟着我一块来配置keepalived吧。 实验场景：在VMware上安装RedHat5.8，内核为Linux-2.6.18，这里模拟实现http服务的高可用，采用两台主机做keepalived高可用，另外两台做http高可用服务集群，安装的系统均为RedHat5.8，内核为Linux-2.6.18。 123keepalived高可用主机IP：172.16.32.30和172.16.32.31http服务高可用主机IP：172.16.32.32和172.16.32.33VIP采用172.16.32.5 这里采用LVS的DR模型来实现集群服务。所以keepalived只需一个网卡即可。假如你想做NAT模型，请添加第二块网卡。LVS的相关理论知识会在以后的博客中介绍。 关闭四台主机的selinux。在命令行界面执行setenforce 0可关闭selinux，否则会对我们的测试有影响。 关闭iptables防火墙。 各虚拟机及主机名和IP对应关系如下所示： 123456虚拟机 主机名 IP地址 HA1 node1.langdu.com 172.16.32.30 HA2 node2.langdu.com 172.16.32.31 HA3 node3.langdu.com 172.16.32.32 HA4 node4.langdu.com 172.16.32.33 知识点补充： 集群(Cluster)类型： 123LB：Load Balancing，负载均衡集群，以提高服务的并发能力为着眼点HA：High Available，高可用集群，以提高服务可用性为着眼点HP(HPC)：High Performance，高性能集群，以提高服务系统处理性能为着眼点 对于高可用集群来说，为避免集群分裂，一个高可用集群至少要有3个节点，或奇数个节点。这里因硬件的限制，我仅用了两个节点，在实际使用中最好使用奇数个节点。既然是高可用集群了，那可用性如何计算呢？这里给出一个计算公式：可用性=在线时间/(在线时间+故障处理时间)可能用到的各IP简写： 1234CIP：Client IPVIP：virtual IPDIP：Director IPRIP：realserver IP LVS类型： 1234LVS-NAT：地址转换LVS-DR：直接路由LVS-TUN：隧道 各类型需遵循的法则： LVS-NAT： 1234567集群节点跟director必须在同一个IP网络中 RIP地址通常是私有地址，仅用于各集群节点间通信 director位于client和reals erver之间，并负责处理进出的所有通信 realserver必须将网关指向DIP 支持端口映射 realserver可以使用任意OS 较大规模应用场景中，director易成为系统瓶颈 LVS-DR： 123456此模型下VIP地址配置在realserver的网卡别名上，通常情况下是隐藏的。 集群节点跟director必须在同一个物理网络中 RIP可以使用公网地址，实现便捷的远程管理和监控 director仅负责处理入站请求，响应报文则由realserver直接发往客户端 realserver不能将网关指向DIP 不支持端口映射 LVS-TUN 123456集群节点可以跨越互联网 RIP必须是公网IP地址 director仅负责处理入站请求，响应报文则由realserver直接发往客户端 realserver不能将网关指向director 只有支持隧道功能的OS才能用于realserver 不支持端口映射 Director调度策略： 静态调度 123456rr：轮叫，又称轮询 wrr：Weight rr，加权轮询 sh：source hashing，源地址hash 实现会话绑定：session affinity ssession sharing：会话共享 dh：destination hash：目标地址hash 动态调度 1234567891011lc：最少连接 active*256+inactive 谁的小，挑谁 wlc：加权最少连接 (active*256+inactive)/weight 谁的小，挑谁 sed：最短期望延迟 (active+1)*256/weight nq：never queue，永不排队 LBLC：基于本地的最少连接 LBLCR：基于本地的带复制功能的最少连接 安装完ipvsadm后默认方法：wlc LVS-DR模型： 123456789kernel parameter：arp_announce：定义将自己的地址向外通告时的通告级别 0：将本机任何接口上的任何地方向外通告(默认为0) 1：试图仅想目标网络通告与其网络匹配的地址 2：仅将与本地接口上地址匹配的网络进行通告arp_ignore：定义接受到ARP请求时的响应级别 0：只要本地配置的有相应地址，就给予响应(默认为0) 1：仅在请求的目标地址配置请求到达的接口上的时候，才给予响应 集群相关基础性知识基本就这些了。还有其他的这里就暂不介绍了，相关更多知识会在以后的集群博客中介绍。这里知识帮助理解。 实现集群的前提： 1231、时间同步，这里采用cron任务计划实现；2、主机名解析，最好通过配置/etc/hosts实现，不要使用DNS来实现，这里通过配置/etc/hosts来实现；3、双机互信。 首先，解决时间同步的问题： 1234service ntpd stop #在四台主机上关闭本机上时间同步的服务，因我们常挂起虚拟机，各虚拟机时间可能不相同，因此我们不使用系统时间同步的服务ntpdate 172.16.0.1#在各主机上执行该命令，同步时间与该地址的时间相同，该地址最好可以上网，这里是与本地网关服务器的时间同步crontab -e #在各主机上使用该命令添加cron任务，每5分钟同步一下时间，同步后不管输出什么信息都送到/dev/null中，否则，你会每5分钟收到一封邮件*/5****/sbin/ntpdate 172.16.0.1&amp;&gt; /dev/null 然后，配置主机名解析：我们先来修改各主机名： 123hostname node1.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.30的主机，修改其主机名HOSTNAME=node1.langdu.com #修改主机名 同样在其他主机上也修改主机名。 1234567891011121314hostname node2.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.31的主机，修改其主机名HOSTNAME=node2.langdu.com #修改主机名hostname node3.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.32的主机，修改其主机名HOSTNAME=node3.langdu.com #修改主机名hostname node4.langdu.com #使用该命令修改主机名，可立即生效，使用logout退出虚拟机后在登录即可看到效果vim /etc/sysconfig/network #编辑IP为172.16.32.33的主机，修改其主机名HOSTNAME=node4.langdu.com #修改主机名vim /etc/hosts #在IP为172.16.32.30的主机上，编辑该文件，添加如下四行信息172.16.32.30node1.langdu.com node1172.16.32.31node2.langdu.com node2172.16.32.32node3.langdu.com node3172.16.32.33node4.langdu.com node4 在node1主机上使用scp命令将该文件传给其他三台主机： 123scp /etc/hosts 172.16.32.31:/etc/#执行该命令时，需输入各主机的密码，可能有点麻烦scp /etc/hosts 172.16.32.32:/etc/scp /etc/hosts 172.16.32.33:/etc/ 最后，实现双机互信：因我们这里是两台主机做keepalived，两台主机做http，因此，我们将前两台实现双机互信，后两台实现双机互信，即node1和node2双机互信，node3和node4双机互信。首先在node1上： 12ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.31#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 接着在node2上： 12ssh-keygen -t rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.30#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 这样，以后在node1和node2之间通信时，我们就不需要输入密码了ssh node2 ‘ifconfig’#在node1主机上，使用该命令，查看下是否已实现双机互信同样在另外两台主机上也需要执行上述命令在node3主机上，执行如下命令： 12ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.33#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 接着在node4上： 12ssh-keygen -t rsa -P &apos;&apos; #在/root目录下执行该命令，生成密钥文件，密码为空ssh-copy-id-i .ssh/id_rsa.pub root@172.16.32.32#使用该命令将该密钥传给另一台主机，身份为root，确保当前处于/root目录下 准备工作已经完毕，现在来安装我们的keepalived软件包。这里我们通过安装rpm包来实现，有兴趣的读者也可以从网上下载源码包，自己编译安装。本rpm是经过源码编译制作的rpm包，相关的脚本及示例配置文件都已制作进来，所以，直接在本地安装即可。假如你是从网上下载的rpm包，里边是没有示例配置文件和相关脚本的，需要自己写。 首先准备好我们的yum源，解决依赖关系时有可能用到里边的相关rpm包。在node1上，使用如下命令完成安装，同样在node2上也需要安装该软件包，因node1和node2实现keepalived高可用。 12345678910yum -y --nogpgcheck localinstall keepalived-1.2.7-5.el5.i386.rpm #因rpm我们以下载至本地，所以采用本地安装，别忘了在另一台主机上也安装该软件包rpm -ql keepalived #使用该命令查看安装的rpm生成哪些文件，这里只贴出生成的部分文件/etc/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.haproxy_example #配置示例文件，网上下载的rpm包安装后没有该文件/etc/keepalived/notify.sh #脚本文件，同样网上下载安装的也没有该脚本文件/etc/rc.d/init.d/keepalived/etc/sysconfig/keepalived/usr/bin/genhash/usr/sbin/keepalived 一起来看下配置文件里的内容： 1234/etc/keepalived/keepalived.conf配置文件中，可分为三个部分：global_defs #全局配置部分vrrp_instance VI_1 #vrrp实例，用来定义虚拟路由组virtual_server 192.168.200.100 443 #虚拟服务部分，用来定义LVS相关配置的 各部分配置的各参数含义： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253global_defs &#123;notification_email &#123; #通知邮件收件人acassen@firewall.locfailover@firewall.locsysadmin@firewall.loc&#125;notification_email_from Alexandre.Cassen@firewall.loc #定义通知邮件的来源smtp_server 192.168.200.1smtp_connect_timeout 30router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; #vrrp实例部分state MASTER #定义初始状态下虚拟路由状态interface eth0 #定义配置在哪个端口上virtual_router_id 51#虚拟路由组ID号priority 100#优先级advert_int 1#每隔1秒进行通告authentication &#123; #实现认证auth_type PASS #采用字符认证auth_pass 1111#认证需要的字符串，最好是随机生成的，两边主机需一样&#125;virtual_ipaddress &#123; #在对应接口上配置虚拟IP，可根据需要进行添加或删除192.168.200.16192.168.200.17192.168.200.18&#125;&#125;virtual_server 192.168.200.100443&#123; #虚拟服务部分，虚拟IP和端口分别是多少，delay_loop 6#获取服务时的等待时间lb_algo rr #集群调度策略，默认为轮询，可自行修改lb_kind NAT #集群转发方式nat_mask 255.255.255.0#虚拟IP的掩码persistence_timeout 50#集群持久连接超时时长，不想支持持久连接可去掉该项protocol TCP #协议为TCPsorry_server 192.168.200.2001358#定义当real_server都down之后，该怎么办real_server 192.168.201.100443&#123; #真实后台服务器IP及端口号，当有多个RIP时可出现多次weight 1#该服务器的权重SSL_GET &#123; #采用SSL进行健康检查，还有其他方式也可实现健康检查url &#123;path /#通过443端口到指定路径下获取相关服务digest ff20ad2481f97b1754ef3e12ecd3a9cc #摘要码&#125;url &#123;path /mrtg/status_code 200#状态码，访问正常时状态码为200&#125;connect_timeout 3#定义多长时间检查一次nb_get_retry 3#检查不健康后，重试次数delay_before_retry 3#多长时间重试一次&#125;&#125;&#125; keepalived配置文件主要内容已经介绍完毕。 想要了解更多keepalived配置文件的信息可使用该命令来查看： 1man keepalived.conf 接下来我们先去配置好http高可用服务两台主机，然后再回来配置keepalived。 既然是LVS的DR模型，那我们先来配置另外两台主机，来实现http高可用集群。 首先，打开另外两外两台提供http服务的虚拟机，这里是HA3和HA4。 为了方便你也可以改为RS1和RS2。先在HA3上执行如下命令： 1234567rpm -q httpd #确保已经安装过httpd软件包，如果没有请自行安装该软件包echo &quot;&lt;h1&gt;RS1.langdu.com&lt;/h1&gt;&quot; &gt; /var/www/html/index.html #为http提供主界面service httpd start #启动服务同样，在HA4虚拟机上也执行同样的命令，但主界面要换成相应的命令。rpm -q httpd #确保已经安装过httpd软件包，如果没有请自行安装该软件包echo &quot;&lt;h1&gt;RS2.langdu.com&lt;/h1&gt;&quot; &gt; /var/www/html/index.html #为http提供主界面service httpd start #启动服务 然后在我们的物理机上尝试访问这两个IP，看是否可以访问。 在DR模型中，只添加个http服务还不行，还有许多需要修改，貌似不是很简单，好吧，为了节约时间，这里我们通过一个脚本实现修改各个数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/bin/bash## Script to start LVS DR real server.# chkconfig: - 90 10# description: LVS DR real server#. /etc/rc.d/init.d/functionsVIP=172.16.32.5#定义VIPhost=`/bin/hostname`case &quot;$1&quot;instart)# Start LVS-DR real server on this machine./sbin/ifconfig lo down/sbin/ifconfig lo upecho 1&gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2&gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho 1&gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho 2&gt; /proc/sys/net/ipv4/conf/all/arp_announce/sbin/ifconfig lo:0$VIP broadcast $VIP netmask 255.255.255.255up/sbin/route add -host $VIP dev lo:0;;stop)# Stop LVS-DR real server loopback device(s)./sbin/ifconfig lo:0downecho 0&gt; /proc/sys/net/ipv4/conf/lo/arp_ignoreecho 0&gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho 0&gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho 0&gt; /proc/sys/net/ipv4/conf/all/arp_announce;;status)# Status of LVS-DR real server.islothere=`/sbin/ifconfig lo:0| grep $VIP`isrothere=`netstat -rn | grep &quot;lo:0&quot;| grep $VIP`if[ ! &quot;$islothere&quot;-o ! &quot;isrothere&quot;];then# Either the route or the lo:0 device# not found.echo &quot;LVS-DR real server Stopped.&quot;elseecho &quot;LVS-DR real server Running.&quot;fi;;*)# Invalid entry.echo &quot;$0: Usage: $0 &#123;start|status|stop&#125;&quot;exit 1;;esac 记得该脚本需要在另一个主机上也要执行一下。 两台主机上都执行过上边的脚本后，验证下各参数是否已经修改：下面几个命令均在node3主机上执行，可在node3上使用ssh node4 ‘COMMAND’来验证下node4上各参数是否已修改 123456789101112131415161718192021222324252627282930ifconfig #在node3上执行该命令，查看是否有VIP，使用ssh node4 &apos;ifconfig&apos;命令查看node4上是否也有VIPeth0 Link encap:Ethernet HWaddr 00:0C:29:7F:8F:44inet addr:172.16.32.33Bcast:172.16.255.255Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1RX packets:162748errors:0dropped:0overruns:0frame:0TX packets:2368errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:1000RX bytes:26502458(25.2MiB) TX bytes:200681(195.9KiB)Interrupt:59Base address:0x2000lo Link encap:Local Loopbackinet addr:127.0.0.1Mask:255.0.0.0UP LOOPBACK RUNNING MTU:16436Metric:1RX packets:10errors:0dropped:0overruns:0frame:0TX packets:10errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:0RX bytes:666(666.0b) TX bytes:666(666.0b)lo:0Link encap:Local Loopbackinet addr:172.16.32.5Mask:255.255.255.255UP LOOPBACK RUNNING MTU:16436Metric:1route -n #查看是否有配置的VIP特定路由Kernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.16.32.50.0.0.0255.255.255.255UH 000lo169.254.0.00.0.0.0255.255.0.0U 000eth0172.16.0.00.0.0.0255.255.0.0U 000eth00.0.0.0172.16.0.10.0.0.0UG 000eth0cat /proc/sys/net/ipv4/conf/all/arp_ignore1cat /proc/sys/net/ipv4/conf/all/arp_announce2 到此，我们的两台RealServer都以配置完毕，而且其http服务也已正常工作。 现在去编辑我们的keepalived的配置文件，并修改成我们所需要的。 先在node1主机上进行修改： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137vim /etc/keepalived/keepalived.confglobal_defs &#123;notification_email &#123;root@localhost #有通告信息时将邮件发给管理员&#125;notification_email_from keepalived@localhost #通告邮件来自哪里smtp_server 127.0.0.1smtp_connect_timeout 30router_id LVS_DEVEL&#125;vrrp_script chk_httpd &#123; #定义该vrrp脚本，上边应提到keepalived就是靠这些脚本来实现其相关功能的script &quot;killall -0 httpd&quot;#尝试杀死该服务，但不是真正杀死该服务，仅为了测试该服务是否启动而已interval 2# check every 2 secondsweight -2# if failed, decrease 2 of the priorityfall 2# require 2 failures for failuresrise 1# require 1 sucesses for ok&#125;vrrp_script chk_schedown &#123; #定义该vrrp脚本，来实现手动转移IP地址，待会演示效果script &quot;[[ -f /etc/keepalived/down ]] &amp;&amp; exit 1 || exit 0&quot;#如果有这个文件，则返回1，否则返回0interval 2weight -2#优先级减2&#125;vrrp_instance VI_1 &#123;state MASTER #定义该主机为master路由interface eth0virtual_router_id 132#定义虚拟路由组ID号，同一网段内不要使用相同的组ID，否则会报错priority 101#定义其优先级advert_int 1authentication &#123;auth_type PASSauth_pass langdu #字符串认证时使用的字符串，可自行修改，但要保证两个keepalived主机上的字符串相同&#125;virtual_ipaddress &#123;172.16.32.5/16dev eth0 label eth0:0#定义VIP，并制定设备和别名&#125;track_script &#123; #健康检查脚本chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master&quot;#如果是master路由器，传递master参数notify_backup &quot;/etc/keepalived/notify.sh backup&quot;#如果是backup路由，传递backup参数notify_fault &quot;/etc/keepalived/notify.sh fault&quot;#如果失败了，传递fault参数&#125;virtual_server 172.16.32.580&#123; #定义虚拟服务器，因我们测试的是http服务，所以端口为80delay_loop 6lb_algo rrlb_kind DRnat_mask 255.255.0.0#该掩码为虚拟服务器的掩码# persistence_timeout 50 #为了待会刷新界面时查看效果，这里我没有启用持久连接，而是将其注释掉了protocol TCPreal_server 172.16.32.3280&#123; #定义RIP和端口号weight 1#权重，在rr调度方式下，该值没有实际意义HTTP_GET &#123; #使用HTTP进行健康检查，假如你使用的是https服务，就需要使用基于SSL的健康检查url &#123;path /status_code 200#状态码&#125;connect_timeout 2nb_get_retry 3delay_before_retry 2&#125;&#125;real_server 172.16.32.3380&#123; #指定另一个RIP和端口号，上边已经提到，当有多个realserver时，该项可以出现多次weight 2HTTP_GET &#123;url &#123;path /status_code 200&#125;connect_timeout 2nb_get_retry 3delay_before_retry 3&#125;&#125;&#125;下面这个脚本时实现健康检查用的。即上边用到的notify.sh脚本。#!/bin/bash# Author: onlyyou# description: An example of notify script#ifalias=$&#123;2:-eth0:0&#125;interface=$(echo $ifalias | awk -F: &apos;&#123;print $1&#125;&apos;)vip=$(ip addr show $interface | grep $ifalias | awk &apos;&#123;print $2&#125;&apos;)contact=&apos;root@localhost&apos;workspace=$(dirname $0)notify() &#123;subject=&quot;$ip change to $1&quot;body=&quot;$ip change to $1 $(date &apos;+%F %H:%M:%S&apos;)&quot;echo $body | mail -s &quot;$1 transition&quot;$contact #实现发送邮件&#125;case &quot;$1&quot;inmaster)notify masterexit 0;;backup)notify backup/etc/rc.d/init.d/httpd restartexit 0;;fault)notify faultexit 0;;*)echo &apos;Usage: $(basename $0) &#123;master|backup|fault&#125;&apos;exit 1;;esacscp /etc/keepalived/keepalived.conf node2:/etc/keepalived/#将配置文件发给另一个keepalived主机，这里发给node2，假如你的主机不是node2，请做相应修改在node2主机上，修改刚传过来的keepalived的配置文件。这里只需修改两项即可：vim /etc/keepalived/keepalived.confvrrp_instance VI_1 &#123;state BACKUP #设置该node2主机为backup路由interface eth0virtual_router_id 132priority 100#设定node2主机的优先级为100，低于node1advert_int 1authentication &#123;auth_type PASSauth_pass langdu&#125;virtual_ipaddress &#123;172.16.32.5/16dev eth0 label eth0:0&#125;track_script &#123;chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master&quot;notify_backup &quot;/etc/keepalived/notify.sh backup&quot;notify_fault &quot;/etc/keepalived/notify.sh fault&quot;&#125; 在node2主机上，我们只需修改上述两项即可。 修改完成后保存退出，接下来就可以启动keepalived服务了。 在上边给的keepalived设计图中我们已经看到，keepalived是为ipvs提供高可用服务的，并且会生成ipvs规则，但需要我们事先安装ipvsadm软件包。好吧，现在我们去安装ipvsadm软件包，然后再启动keepalived服务。 12ssh node2 &apos;yum -y install ipvsadm&apos; #通过node1主机在node2上安装ipvsadm软件包yum -y install ipvsadm #在node1上安装软件包 安装完毕后启动我们的keepalived服务。 12service keepalived start #启动node1上的keepalived服务ssh node2 &apos;service keepalived start&apos; #在node1上启动node2的keepalived服务 我们先来看下我们的日志，看有没有记录什么信息。 12345678910111213141516171819202122232425262728293031323334353637383940tail /var/log/messages #查看日志May 1618:19:22node1 Keepalived_vrrp[789]: Using LinkWatch kernel netlink reflector...May 1618:19:22node1 Keepalived_vrrp[789]: VRRP sockpool: [ifindex(2), proto(112), fd(11,12)]May 1618:19:22node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Transition to MASTER STATE #传输master状态May 1618:19:22node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Received lower prio advert, forcing new electionMay 1618:19:23node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1618:19:23node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) setting protocol VIPs. #设置VIP地址May 1618:19:23node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1618:19:23node1 Keepalived_vrrp[789]: Netlink reflector reports IP 172.16.32.5addedMay 1618:19:23node1 Keepalived_healthcheckers[788]: Netlink reflector reports IP 172.16.32.5addedMay 1618:19:28node1 Keepalived_vrrp[789]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5从上边的日志可以看到，我们的配置已经生效。那一起看下ipvs规则吧。ipvsadm -l -n #使用该命令，查看是否有ipvs规则，显示如下：IP Virtual Server version 1.2.1(size=4096)Prot LocalAddress:Port Scheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 172.16.32.5:80rr-&gt; 172.16.32.33:80Route 100-&gt; 172.16.32.32:80Route 100ifconfig #查看下node1上的IP配置情况eth0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AFinet addr:172.16.32.30Bcast:172.16.255.255Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1RX packets:290653errors:1dropped:0overruns:0frame:0TX packets:13874errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:1000RX bytes:46393162(44.2MiB) TX bytes:2014631(1.9MiB)Interrupt:59Base address:0x2000eth0:0Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AFinet addr:172.16.32.5Bcast:0.0.0.0Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1Interrupt:59Base address:0x2000lo Link encap:Local Loopbackinet addr:127.0.0.1Mask:255.0.0.0UP LOOPBACK RUNNING MTU:16436Metric:1RX packets:10errors:0dropped:0overruns:0frame:0TX packets:10errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:0RX bytes:666(666.0b) TX bytes:666(666.0b) 配置的VIP也已生效，我们的ipvs规则已经实现。现在在我们的物理机上访问下172.16.32.5，看一下，是否可以访问，显示什么信息吧。 到目前为止，貌似我们还没有实现IP地址漂移。好吧，从上边给的配置文件，可以看出来，我们只需在master路由主机上，在相应目录下创建一个down文件即可实现手动漂移IP地址。 在node1上 1234567891011121314151617181920212223242526272829303132333435363738394041cd /etc/keepalived/#进入该目录touch down #创建该文件，用来实现手动漂移IP地址tail /var/log/messages #停几秒钟后，查看日志May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Entering MASTER STATEMay 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) setting protocol VIPs.May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1619:15:32node1 Keepalived_healthcheckers[2815]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:32node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Received higher prio advert #收到更高优先级的通告信息May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Entering BACKUP STATE #进入backup状态May 1619:15:32node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) removing protocol VIPs. #转移VIP地址May 1619:15:32node1 Keepalived_healthcheckers[2815]: Netlink reflector reports IP 172.16.32.5removedMay 1619:15:32node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5removedifconfig #使用该命令，查看下node1主机的VIP是否存在，可看到已转移到其他主机eth0 Link encap:Ethernet HWaddr 00:0C:29:9F:2F:AFinet addr:172.16.32.30Bcast:172.16.255.255Mask:255.255.0.0UP BROADCAST RUNNING MULTICAST MTU:1500Metric:1RX packets:347881errors:1dropped:0overruns:0frame:0TX packets:21333errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:1000RX bytes:50760780(48.4MiB) TX bytes:2653767(2.5MiB)Interrupt:59Base address:0x2000lo Link encap:Local Loopbackinet addr:127.0.0.1Mask:255.0.0.0UP LOOPBACK RUNNING MTU:16436Metric:1RX packets:10errors:0dropped:0overruns:0frame:0TX packets:10errors:0dropped:0overruns:0carrier:0collisions:0txqueuelen:0RX bytes:666(666.0b) TX bytes:666(666.0b)在node2主机上查看其日志。tail /var/log/messages #查看node2的日志信息May 1619:15:32node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:15:32node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:15:33node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 1619:15:34node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1619:15:34node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) setting protocol VIPs. #设定VIPMay 1619:15:34node1 Keepalived_healthcheckers[2463]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:34node1 avahi-daemon[3375]: Registering new address record for172.16.32.5on eth0.May 1619:15:34node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1619:15:34node1 Keepalived_vrrp[2464]: Netlink reflector reports IP 172.16.32.5addedMay 1619:15:39node1 Keepalived_vrrp[2464]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5 在在我们的物理机访问下，看能否访问。依然在浏览器地址栏输入172.16.32.5，可以看到，访问正常。现在，我们删掉/etc/keepalived/down这个文件，看能否实现IP漂移回来。 12345678910111213rm /etc/keepalived/down #删除node1主机上该文件rm: remove regular empty file`down&apos;? ytail /var/log/messages #查看日志信息May 1619:15:32node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5removedMay 1619:27:54node1 Keepalived_vrrp[2816]: VRRP_Script(chk_schedown) succeededMay 1619:27:55node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:27:55node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1619:27:56node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 1619:27:57node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1619:27:57node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) setting protocol VIPs.May 1619:27:57node1 Keepalived_vrrp[2816]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1619:27:57node1 Keepalived_healthcheckers[2815]: Netlink reflector reports IP 172.16.32.5addedMay 1619:27:57node1 Keepalived_vrrp[2816]: Netlink reflector reports IP 172.16.32.5added 至此，我们已成功实现了keepalived的相关功能。以上演示的仅仅是主从模式下地址漂移。那我们能否实现在双主模式下实现地址漂移呢？答案是肯定的。 实现双主模式配置： 在node1主机上，修改keepalived配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647vim /etc/keepalived/keepalived.conf #其他信息不变，仅修改如下信息vrrp_instance VI_2 &#123; #添加虚拟路由组2state BACKUP #定义该路由在虚拟路由组2中为backup路由interface eth0virtual_router_id 232#定义组IDpriority 200#定义在该组中的优先级advert_int 1authentication &#123;auth_type PASSauth_pass langdu&#125;virtual_ipaddress &#123;172.16.32.6/16dev eth0 label eth0:1#因是双主模型，定义VIP及设备和别名，同上边定义的别名要区分开&#125;track_script &#123;chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master eth0:1&quot;#修改这三项，因我们有意定义成其他别名notify_backup &quot;/etc/keepalived/notify.sh backup eth0:1&quot;notify_fault &quot;/etc/keepalived/notify.sh fault eth0:1&quot;&#125;同时注释掉virtual_server部分。在双主模型下，我们不使用virtual_server部分。scp /etc/keepalived/keepalived.conf node2:/etc/keepalived/#将该配置文件传给另一个主机，即node2在node2上，修改keepalived配置文件vim /etc/keepalived/keepalived.confvrrp_instance VI_2 &#123; #在该主机上修改虚拟路由组2state MASTER #修改在该组中本路由为master路由interface eth0virtual_router_id 232#定义组IDpriority 201#修改在该组中的优先级，一定要高于node1中虚拟路由组2的优先级advert_int 1authentication &#123;auth_type PASSauth_pass langdu&#125;virtual_ipaddress &#123;172.16.32.6/16dev eth0 label eth0:1#因是双主模型，定义VIP及设备和别名，同上边定义的别名要区分开&#125;track_script &#123;chk_httpdchk_schedown&#125;notify_master &quot;/etc/keepalived/notify.sh master eth0:1&quot;notify_backup &quot;/etc/keepalived/notify.sh backup eth0:1&quot;notify_fault &quot;/etc/keepalived/notify.sh fault eth0:1&quot;&#125; 在双主模型下，为了让同一个域名解析到不同的IP上，我们需要用到DNS将其解析到不同的两个IP上。这里，为了查看效果，我们先不安装DNS解析。待会在实现。 以上配置好后，重启keepalived服务。在物理机上在浏览器地址栏分别输入172.16.32.5和172.16.32.6查看显示效果。 可以看到，显示正常。我们的配置是正确的。 在我们的node1主机上，我们来手动实现地址漂移，看能否实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293cd /etc/keepalived #进入该目录touch down #创建该文件tail /var/log/messages #查看日志信息May 1621:25:25node1 named[16646]: zone localhost/IN: loaded serial 0May 1621:25:25node1 named[16646]: zone managed-keys.bind/IN/_meta: loaded serial 10May 1621:25:25node1 named[16646]: runningMay 1621:25:25node1 named[16646]: zone langdu.com/IN: sending notifies (serial 2013005)May 1621:45:59node1 Keepalived_vrrp[11785]: VRRP_Script(chk_schedown) failedMay 1621:46:00node1 Keepalived_vrrp[11785]: VRRP_Instance(VI_1) Received higher prio advertMay 1621:46:00node1 Keepalived_vrrp[11785]: VRRP_Instance(VI_1) Entering BACKUP STATE #可看到进入backup状态May 1621:46:00node1 Keepalived_vrrp[11785]: VRRP_Instance(VI_1) removing protocol VIPs. #VIP漂移成功May 1621:46:00node1 Keepalived_vrrp[11785]: Netlink reflector reports IP 172.16.32.5removedMay 1621:46:00node1 Keepalived_healthcheckers[11784]: Netlink reflector reports IP 172.16.32.5removed现在，一起看下node2上的日志信息：tail /var/log/messagesMay 1621:46:00node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1621:46:00node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) forcing a new MASTER electionMay 1621:46:01node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Transition to MASTER STATEMay 1621:46:02node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Entering MASTER STATE #进入master状态May 1621:46:02node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) setting protocol VIPs.May 1621:46:02node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5May 1621:46:02node1 Keepalived_vrrp[11537]: Netlink reflector reports IP 172.16.32.5added #添加另一个VIPMay 1621:46:02node1 Keepalived_healthcheckers[11536]: Netlink reflector reports IP 172.16.32.5addedMay 1621:46:02node1 avahi-daemon[3375]: Registering new address record for172.16.32.5on eth0.May 1621:46:07node1 Keepalived_vrrp[11537]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth0 for172.16.32.5现在我们来安装DNS服务器，实现将同一个域名解析到不同的IP地址上。这里我们将node1作为DNS服务器来负责域名解析。因此，以下命令在node1主机上执行。yum -y install bind97 bind97-utils #安装bind97，来提供DNS服务，在安装前请确保你的虚拟机上没有安装其他bind软件包vim /etc/named.rfc1912.zones #编辑该文件，添加如下信息zone &quot;langdu.com&quot;IN &#123; #添加该区域，DNS相关配置请查看我的相关博客，里边有详细介绍typemaster; file&quot;langdu.com.zone&quot;;&#125;;zone &quot;32.16.172.in-addr.arpa&quot;IN &#123;typemaster;file&quot;172.16.32.zone&quot;;&#125;;cd /etc/namedvim 172.16.32.zone#编辑该文件，添加如下内容：$TTL 600@ IN SOA ns.langdu.com. admin.langdu.com. (20130054H5M3D1D)IN NS ns.langdu.com.30IN PTR ns.langdu.com.6IN PTR www.langdu.com.5IN PTR www.langdu.com.vim langdu.com.zone #编辑该文件，添加如下内容：$TTL 600@ IN SOA ns.langdu.com. admin.langdu.com. (20130054H5M3D1D)IN NS nsIN NS wwwIN NS wwwns IN A 172.16.32.30www IN A 172.16.32.5#实现将同一个域名解析到不同的IP上www IN A 172.16.32.6named-checkconf #检查配置文件named-checkzone &quot;langdu.com&quot;/var/named/langdu.com.zone #检查区域文件named-checkzone &quot;32.16.172.in-addr.arpa&quot;/var/named/172.16.32.zone在我们的物理机上修改hosts文件。打开C:\Windows\System32\drivers\etc/hosts文件，添加两行信息：172.16.32.5www.langdu.com172.16.32.6www.langdu.com检查没有问题后启动服务。service named startdig -t A www.langdu.com @172.16.32.30#使用该命令查询下DNS服务器是否可用，以下是显示结果：; &lt;&lt;&gt;&gt; DiG 9.7.0-P2-RedHat-9.7.0-6.P2.el5_7.4&lt;&lt;&gt;&gt; -t A www.langdu.com @172.16.32.30;; globaloptions: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;-opcode: QUERY, status: NOERROR, id: 5351;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 2, ADDITIONAL: 1;; QUESTION SECTION:;www.langdu.com. IN A;; ANSWER SECTION:www.langdu.com. 600IN A 172.16.32.5#解析成功，可看到同一域名解析出两个IP地址www.langdu.com. 600IN A 172.16.32.6;; AUTHORITY SECTION:langdu.com. 600IN NS ns.langdu.com.langdu.com. 600IN NS www.langdu.com.;; ADDITIONAL SECTION:ns.langdu.com. 600IN A 172.16.32.30;; Query time: 5msec;; SERVER: 172.16.32.30#53(172.16.32.30);; WHEN: Thu May 1621:42:072013;; MSG SIZE rcvd: 111 我们的DNS服务器也做好了。现在在物理机上的浏览器地址栏输入www.langdu.com，查看下显示效果。因为是DNS服务器解析得到的，因此，刷新多次后可能还是显示同一个界面内容。 原文引自这里]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本替换文件中某个字符串]]></title>
    <url>%2F2017%2F12%2F13%2Fshell%E8%84%9A%E6%9C%AC%E6%9B%BF%E6%8D%A2%E6%96%87%E4%BB%B6%E4%B8%AD%E6%9F%90%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[将当前目录下包含jack串的文件中，jack字符串替换为tom 1sed -i &quot;s/jack/tom/g&quot; `grep &quot;jack&quot; -rl ./` 将某个文件中的jack字符串替换为tom 1sed -i &quot;s/jack/tom/g&quot; test.txt linux sed 批量替换多个文件中的字符串 1sed -i &quot;s/oldstring/newstring/g&quot; `grep oldstring -rl yourdir` 例如：替换/home下所有文件中的www.bcak.com.cn为bcak.com.cn 1sed -i &quot;s/www.bcak.com.cn/bcak.com.cn/g&quot; `grep www.bcak.com.cn -rl /home` 下面这条命令： 1perl -pi -e &apos;s|ABCD|Linux|g&apos; `find ./ -type f` 将调用perl执行一条替换命令，把find命令找到的所有文件内容中的ABCD替换为Linux 1find ./ -type f 此命令是显示当前目录下所有的文件,上面的 “s|ABCD|Linux| g” 是perl要执行的脚本，即把所有ABCD替换为Linux,如果不写最后的那个g，“s|ABCD|Linux| ”将只替换每一行开头的ABCD]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三大文本处理工具grep与sed及awk简介(转)]]></title>
    <url>%2F2017%2F12%2F13%2F%E4%B8%89%E5%A4%A7%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7grep%E4%B8%8Esed%E5%8F%8Aawk%E7%AE%80%E4%BB%8B-%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[grep、sed和awk都是文本处理工具，虽然都是文本处理工具单却都有各自的优缺点，一种文本处理命令是不能被另一个完全替换的，否则也不会出现三个文本处理命令了。只不过，相比较而言，sed和awk功能更强大而已，且已独立成一种语言来介绍。 grep：==文本过滤器==，如果仅仅是 过滤文本，可使用grep，其效率要比其他的高很多； sed：Stream EDitor，==流编辑器==，默认只处理模式空间，不处理原数据，如果你处理的数据是 针对行进行处理的，可以使用sed； awk：==报告生成器==，格式化以后显示。如果对处理的数据需要生成报告之类的信息，或者 你处理的数据是按列进行处理的 ，最好使用awk。 grepgrep是一个最初用于Unix操作系统的命令行工具。在给出文件列表或标准输入后，grep会对匹配一个或多个正则表达式的文本进行搜索，并只输出匹配（或者不匹配）的行或文本。 Unix的grep家族包括grep、egrep和fgrep。 egrep和fgrep的命令只跟grep有很小不同。egrep是grep的扩展，支持更多的re元字符，fgrep就是fixed grep或fastgrep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。 linux使用GNU版本的grep。它功能更强，可以通过-E、-F命令行选项来使用egrep和fgrep的功能。 grep的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到屏幕，不影响原文件内容。 grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 grep：根据模式搜索文本，并将符合模式的文本行显示出来。 Pattern：文本字符和正则表达式的元字符组合而成匹配条件 123456789101112131415161718192021222324使用格式：grep [options] PATTERN [FILE...] -i：忽略大小写 --color：匹配到字符用其他颜色显示出来，默认是红色 -v：显示没有被模式匹配到的行 -o：只显示被模式匹配到的字符串，不显示行 -E：使用扩展正则表达式 -A n：表示显示该行及其后n行 -B n：表示显示该行及其前n行 -C n：表示显示该行及其前后各n行 正则表达式：REGular EXPression，REGEXP元字符：.：匹配任意单个字符[]：匹配指定范围内的任意单个字符[^]：匹配指定范围外的任意单个字符 字符集和：[:digit:]，[:lower:]，[:upper:]，[:punct:]，[:space:]，[:alpha:]，[:alnum:] 对应上边：数字 ，小写字母，大写字母，标点符号，空白字符，所有字母，所有数字和字母匹配次数（贪婪模式，即尽可能长的匹配）：*：匹配其前面的字符任意次 匹配.b和.*b，看二者有什么区别，命令和显示效果如下： grep练习： 12345678910111213141516171819202122232425262728293031323334353637381. 显示/proc/meminfo文件中以不区分大小的s开头的行； grep -i &apos;^s&apos; /proc/meminfo 或者 grep &apos;^[sS]&apos; /proc/meminfo #[]表示匹配指定范围内的单个字符，因此也可实现不区分大小写 2. 显示/etc/passwd中以nologin结尾的行; grep &apos;nologin$&apos; /etc/passwd 扩展一：取出默认shell为/sbin/nologin的用户列表 grep &apos;/sbin/nologin&apos; /etc/passwd | cut -d: -f1 或者 grep &apos;/sbin/nologin&apos; /etc/passwd | awk -F: &apos;&#123;print $1&#125;&apos; 或者直接使用awk awk -F: &apos;$7 ~ /nologin/&#123;print $1&#125;&apos; /etc/passwd 扩展二：取出默认shell为bash，且其用户ID号最小的用户的用户名 grep &apos;bash$&apos; /etc/passwd | sort -n -t: -k3 | head -1 | cut -d: -f1 或者 awk -F: &apos;$7 ~ /bash/&#123;print $3,$1&#125;&apos; /etc/passwd | sort -n | head -1 | awk &apos;&#123;print $2&#125;&apos;3. 显示/etc/inittab中以#开头，且后面跟一个或多个空白字符，而后又跟了任意非空白字符的行； grep &apos;^#[[:space:]]\&#123;1,\&#125;[^[:space:]]&apos; /etc/inittab4. 显示/etc/inittab中包含了:一个数字:(即两个冒号中间一个数字)的行； grep &apos;:[0-9]:&apos; /etc/inittab5. 显示/boot/grub/grub.conf文件中以一个或多个空白字符开 头的行； grep &apos;^[[:space:]]\&#123;1,\&#125;&apos; /boot/grub/grub.conf6. 显示/etc/inittab文件中以一个数字开头并以一个与开头数字相同的数字结尾的行； grep &apos;\(^[0-9]\).*\1$&apos; /etc/inittab #在RHEL5.8以前的版本中可查看到效果7. 找出某文件中的，1位数，或2位数； grep &apos;\&lt;[[:digit:]][[:digit:]]\?\&gt;&apos; /etc/inittab 或者 grep &apos;\&lt;[0-9]\&#123;1,2\&#125;\&gt;&apos; /etc/inittab8. 查找当前系统上名字为student(必须出现在行首)的用户的帐号的相关信息, 文件为/etc/passwd grep &apos;^student:&apos; /etc/passwd 扩展：若存在该用户，找出该用户的ID号： grep &apos;^student:&apos; /etc/passwd | cut -d: -f3 或者# id -u student ``` 思考题：用多种方法找出本地的IP地址，这里给出三种方法，如果你还有其他方法可以一起分享下：``` ifconfig eth0|grep -oE &apos;([0-9]&#123;1,3&#125;\.?)&#123;4&#125;&apos;|head -n 1ifconfig eth0|awk -F: &apos;/inet addr/&#123;split($2,a,&quot; &quot;);print a[1];exit&#125;&apos; #这里使用了awk的内置函数，如果不懂可在看完awk的介绍后再来做此题ifconfig |grep &quot;inet addr&quot;|grep -v &quot;127.0.0.1&quot; |awk -F: &apos;&#123;print $2&#125;&apos; |awk &apos;&#123;print $1&#125;&apos; sedsed（意为流编辑器，源自英语“stream editor”的缩写）是Unix常见的命令行程序。 sed 用来把文档或字符串里面的文字经过一系列编辑命令转换为另一种格式输出。 sed 通常用来匹配一个或多个正则表达式的文本进行处理。sed是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等 sed是一个精简的、非交互式的编辑器。它能执行与编辑vi和emacs相同的编辑任务，但sed编辑器不提供交互使用方式，只能在命令行下输入编辑命令。 作为编辑器，当然少不了插入（a/、i/）、删除（d）、查找替换(s)等命令。 sed练习 删除/etc/grub.conf文件中行首的空白符； 1sed -r &apos;s/^[[:space:]]+//&apos; /etc/grub.conf 替换/etc/inittab文件中“id:3:initdefault:”一行中的数字为5； 1sed &apos;s/\(id:\)[0-9]\(:initdefault:\)/\15\2/g&apos; /etc/inittab 删除/etc/inittab文件中的空白行； 1sed &apos;/^$/d&apos; /etc/inittab 删除/etc/inittab文件中开头的#号； 1sed &apos;s/^#//g&apos; /etc/inittab 删除某文件中开头的#号及其后面的空白字符，但要求#号后面必须有空白符； 1sed &apos;s/^#[[:space:]]\&#123;1,\&#125;//g&apos; /etc/inittab 或 sed -r &apos;s/^#[[:space:]]+//g&apos; /etc/inittab 删除某文件中以空白字符后面跟#类的行中的开头的空白字符及# 1sed -r &apos;s/^[[:space:]]+#//g&apos; /etc/inittab 取出一个文件路径的目录名称; 1echo &quot;/etc/rc.d/abc/edu/&quot; | sed -r &apos;s@^(/.*/)[^/]+/?@\1@g&apos; 因sed支持扩展正则表达式，在扩展正则表达式中，+表示匹配其前面的字符至少1次 取出一个文件路径的最后一个文件名； 1echo &quot;/etc/rc.d/abc/edu/&quot; | sed -r &apos;s@^/.*/([^/]+)/?@\1@g&apos; awkawk是一种优良的文本处理工具，Linux及Unix环境中现有的功能最强大的数据处理引擎之一。 这种编程及数据操作语言（其名称得自于它的创始人Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母）的最大功能取决于一个人所拥有的知识。 AWK提供了极其强大的功能：可以进行正则表达式的匹配，样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数。它具备了一个完整的语言所应具有的几乎所有精美 特性。 实际上AWK的确拥有自己的语言：AWK程序设计语言，三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。最简单地说，AWK是一种用于处理文本的编程语言工具。 我们现在使用最多的是gawk，gawk是AWK的GNU版本。 AWK的功能是什么？ 与sed和grep很相似，awk是一种样式扫描与处理工具。但其功能却大大强于sed和grep。awk提供了极其强大的功能：它几乎可以完成grep和sed所能完成的全部工作，同时，它还可以可以进行样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数。它具备了一个完整的语言所应具有的几乎所有精美特性。 awk的命令格式为：awk是一种程序语言，对文档资料的处理具有很强的功能。awk擅长从格式化报文或从一个大的文本文件中抽取数据。 1awk [-F filed-separator] “commands” input-file(s) 每一个命令（commands）都由两部分组成：一个模式（pattern）和一个相应的动作（action） 123/pattern1/&#123;action1&#125; /pattern2/&#123;action2&#125; /pattern3/&#123;action3&#125; awk将一行文字按分隔符（filed-separator）分为多个域，依次记为$ 1，$ 2 . . . $ n。$0代表所有域值。 因此awk更适合于以域为单位来处理文件。加之ARGIND等内置变量，使awk能处理多个文件。典型的应用为查找一个文件中的某个字段是否在另一个文件中出现。 但由于$0代表所有域，即整行，因此awk也有简单行处理能力。 awk的输出：print和printf print print的使用格式： print item1, item2, … 要点： 各项目之间使用逗号隔开，而输出时则以空白字符分隔； 输出的item可以为字符串或数值、当前记录的字段(如$1)、变量或awk的表达式；数值会先转换为字符串，而后再输出； print命令后面的item可以省略，此时其功能相当于print $0, 因此，如果想输出空白行，则需要使用print “”； 例子： 123awk &apos;BEGIN &#123; print &quot;line one\nline two\nline three&quot; &#125;&apos;awk -F: &apos;&#123; print $1, $7 &#125;&apos; /etc/passwd #等价于：awk -v FS=: &apos;&#123;print $1,$7&#125;&apos; /etc/passwd awk变量 awk内置变量之记录变量： 1234FS: field separator，字段分隔符，默认是空白字符；RS: Record separator，记录分隔符，默认是换行符；OFS: Output Filed Separator，输出字段分隔符ORS：Output Row Separator，输出行分隔符 一起来看一个示例： 12345vim test.txt #编辑该文件，添加如下两行信息作为示例使用welcome to redhat linux.how are you?[root@www ~]# awk &apos;BEGIN&#123;OFS=&quot;#&quot;&#125; &#123;print $1,$2&#125;&apos; test.txt #指定输出时的分隔符[root@www ~]# awk &apos;BEGIN&#123;OFS=&quot;#&quot;&#125; &#123;print $1,&quot;hello&quot;,$2&#125;&apos; test.txt #指定输出时的分隔符，并添加显示的内容 awk内置变量之数据变量： 123456789NR: The number of input records，awk命令所处理的记录数；如果有多个文件，这个数目会把处理的多个文件中行统一计数；NF：Number of Field，当前记录的字段个数，有时可用来表示最后一个字段FNR: 与NR不同的是，FNR用于记录正处理的行是当前这一文件中被总共处理的行数；ARGV: 数组，保存命令行本身这个字符串，如awk &apos;&#123;print $0&#125;&apos; a.txt b.txt这个命令中，ARGV[0]保存awk，ARGV[1]保存a.txt；ARGC: awk命令的参数的个数；FILENAME: awk命令所处理的文件的名称；ENVIRON：当前shell环境变量及其值的关联数组；如： 12awk &apos;BEGIN&#123;print ENVIRON[&quot;PATH&quot;]&#125;&apos;awk &apos;&#123;print $NF&#125;&apos; test.txt gawk命令也可以在“脚本”外为变量赋值，并在脚本中进行引用。例如，上述的例子还可以改写为： 123[root@www ~]# awk -v var=&quot;variable testing&quot; &apos;BEGIN&#123;print var&#125;&apos; #与上述的例子一样，显示效果如下variable testing printf printf命令的使用格式：printf format, item1, item2, … 要点： 1231. 其与print命令的最大不同是，printf需要指定格式；2. format用于指定后面的每个item的输出格式；3. printf语句不会自动打印换行，需要显式使用\n换行。 format格式的指示符都以%开头，后跟一个字符；如下：%c: 显示字符的ASCII码；%d, %i：十进制整数；%e, %E：科学计数法显示数值；%f: 显示浮点数；%g, %G: 以科学计数法的格式或浮点数的格式显示数值；%s: 显示字符串；%u: 无符号整数；%%: 显示%自身； 1awk -F: &apos;&#123;printf &quot;%-15s%i\n&quot;,$1,$3&#125;&apos; /etc/passwd 使用printf显示该文件中的第一列和第三列，要求第一列左对齐且占用15个字符宽度，第二列显示十进制整数，显示效果如下所示： 字符串操作符： 只有一个，而且不用写出来，用于实现字符串连接； 12[root@www ~]# awk &apos;BEGIN&#123;print &quot;A&quot; &quot;B&quot;&#125;&apos; #连接A和B两个字符，使其成为一个字符串，显示效果如下所示：AB 总结： 如果文件是格式化的，即由分隔符分为多个域的，优先使用awk awk适合按列（域）操作，sed适合按行操作 awk适合对文件的抽取整理，sed适合对文件的编辑。 grep 主要用于搜索某些字符串 sed，awk 用于处理文本 原文引自]]></content>
      <categories>
        <category>善用佳软</category>
      </categories>
      <tags>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix3.4安装部署]]></title>
    <url>%2F2017%2F12%2F12%2Fzabbix3-4%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[运行环境： 12345nginx-1.12.1mysql-5.5.56php-7.1.7Ubuntu 16.04Linux localhost 4.9.50-x86_64 前言Linux下常用的系统监控软件有Nagios、Cacti、Zabbix、Monit等，这些开源的软件，可以帮助我们更好的管理机器，在第一时间内发现，并警告系统维护人员。 使用Zabbix的目的，是为了能够更好的监控mysql数据库服务器，并且能够生成图形报表，虽然Nagios也能够生成图形报表，但没有Zabbix这么强大。 Zabbix简介 zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。 zabbix由zabbix server与可选组件zabbix agent两部门组成。 zabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器/网络状态的监视。 zabbix agent需要安装在被监视的目标服务器上，它主要完成对硬件信息或与操作系统有关的内存，CPU等信息的收集。 zabbix的主要特点： 安装与配置简单，学习成本低 支持多语言（包括中文） 免费开源 自动发现服务器与网络设备 分布式监视以及WEB集中管理功能 可以无agent监视 用户安全认证和柔软的授权方式 通过WEB界面设置或查看监视结果 email等通知功能 Zabbix主要功能 CPU负荷 内存使用 磁盘使用 网络状况 端口监视 日志监视 Zabbix安装 zabbix WEB环境搭建 zabbix的安装需要LAMP或者LNMP环境。 2.zabbix 数据库设置 zabbix数据库可以和zabbix服务器分离，采用用专门的mysql服务器存储数据，此时要给zabbix数据库受相应的权限。 对于Zabbix server 和 proxy 守护进程以及Zabbix前端，都需要连接到一个数据库。Zabbix agent不需要数据库的支持。 SQL 脚本 用于创建数据库架构（schema）并插入数据集（dataset）。 Zabbix proxy数据库只需要数据库架构（schema），而Zabbix server数据库在建立数据库架构（schema）后，还需要数据集（dataset）。 建立Zabbix数据库后，可以开始对Zabbix进行编译。 1grant all privileges on zabbix.* to zabbix_user@&apos;ip&apos; identified by &apos;123456&apos;; == 注：ip为zabbix服务器的IP地址。== 关于数据库的安装，可以查看Mysql安装，我习惯使用二进制包。 启动数据库 1/usr/local/mysql/bin/mysqld_safe --user=mysql &amp; 登录数据库，创建帐号和设置权限： 123mysql&gt; use mysql;mysql&gt;create database zabbix character set utf8;mysql&gt;grant all privileges on zabbix.* to zabbix_user@&apos;localhost&apos; identified by &apos;123456&apos;; 安装zabbix服务 增加zabbix用户和组 对于所有Zabbix的守护进程，需要一个无特权的用户。如果Zabbix守护进程以一个无特权的用户账户启动，那么它会使用该用户运行。 然而，如果一个守护进程以‘root’用户启动，它会切换为‘zabbix’用户账户，且这个用户必须存在。在Linux系统中，可以使用下面命令建立一个用户（该用户属于自己的用户组，“zabbix”）： 12#groupadd zabbix#useradd -g zabbix -m zabbix 使用root，bin或其他特殊权限的账户运行Zabbix是一个安全风险。 对于Zabbix前端的安装，不需要使用单独的用户账户。 如果Zabbix server 和 agent 运行在同一台计算机上，建议使用不同的账户运行Server和Agent。否则，如果两个进程使用了同一个用户，Agent就可以访问Server的配置文件，并可轻易地读取Zabbix中任何管理员级别的用户，比如数据库密码。 官网下载解压软件包。 在 Ubuntu 14.04 LTS 上安装 Zabbix 3.4： 1wget http://repo.zabbix.com/zabbix/3.4/ubuntu/pool/main/z/zabbix/zabbix_3.4.4.orig.tar.gz 导入数据库表 12345create user &apos;zabbix&apos;@&apos;localhost&apos; identified by &apos;PASSWORD&apos;;create database zabbix;grant all privileges on `zabbix`.* to &apos;zabbix&apos;@&apos;localhost&apos;;flush privileges;exit; 1234567891011121314151617#cd zabbix-3.4/database/mysqlmysql -u root -p mysql&gt; use zabbix; #进入数据库，按照顺序进行导入，否则会出错。Database changedmysql&gt; source /root/zabbix-3.4.4/database/mysql/schema.sql...Query OK, 0 rows affected (0.05 sec)Records: 0 Duplicates: 0 Warnings: 0 mysql&gt; source /root/zabbix-3.4.4/database/mysql/images.sql... Query OK, 1 row affected (0.01 sec) mysql&gt; source /root/zabbix-3.4.4/database/mysql/data.sql 编译安装zabbix 1./configure --prefix=/usr/local/zabbix --with-mysql=/usr/local/mysql/bin/mysql_config --with-net-snmp --with-libcurl --enable-server --enable-agent --enable-proxy 添加服务端口 12345vim /etc/serviceszabbix-agent 10050/tcp # Zabbix Agentzabbix-agent 10050/udp # Zabbix Agentzabbix-trapper 10051/tcp # Zabbix Trapperzabbix-trapper 10051/udp # Zabbix Trapper 添加配置文件 123mkdir -p /etc/zabbixcp -r zabbix-3.4/conf/* /etc/zabbix/chown -R zabbix:zabbix /etc/zabbix 修改server配置文件，添加zabbix数据库密码 1vim /etc/zabbix/zabbix_server.conf 1234567891011LogFile=/tmp/zabbix_server.logPidFile=/tmp/zabbix_server.pidDBName=zabbixDBUser=zabbix_userDBPassword=123456 #指定zabbix数据库密码ListenIP=192.168.10.197 #服务器IP地址 修改Agentd配置文件，更改HOSTNAME为本机的hostname 1vim /etc/zabbix/zabbix_agentd.conf 123456789PidFile=/tmp/zabbix_agentd.pid #进程PIDLogFile=/tmp/zabbix_agentd.log #日志保存位置EnableRemoteCommands=1 #允许执行远程命令Server=192.168.10.197 #agent端的ipHostname=client1 #必须与zabbix创建的host name相同 添加web前段php文件 123cd zabbix-3.4/frontends/cp -rf php /home/httpd/zabbix #虚拟主机目录chown -R zabbix:zabbix zabbix web前端安装配置 修改PHP相关参数 1vim php.ini 123456max_execution_time = 300max_input_time = 300memory_limit = 128Mpost_max_size = 32Mdate.timezone = Asia/Shanghaimbstring.func_overload=2 PHP还必须支持一下模块，在php源码包直接编译安装。详细模块需要在安装是会提示:bcmath.so、gettext.so 在客户端浏览器上面访问zabbix，开始WEB的前端配置，http://ZabbixIP/zabbix 按提示点击下一步 Step1：下一步。Step2：如果全部OK的话才能进行下一步的安装，如果有错误请返回到server端检查相关的软件包是否安装。Step3：需要输入mysql数据库帐号密码,如果数据库不在zabbix服务器上面，在Host里面添加数据库服务器的地址，并且要用grant命令给数据库授权。Step4：输入服务器端 host name or host IP addres； 最后会自动写入配置文件：zabbix.conf.php，配置完成后出现登陆界面，默认的用户名为：admin，密码为：zabbix。 Now we’ll add a user for Zabbix to run as. We’ll configure scripts to control the zabbix server daemon. 1- Create the file /etc/systemd/system/multi-user.target.wants/zabbix-server.service 12345678910111213[Unit]Description=Zabbix ServerAfter=syslog.target network.target mysqld.service[Service]Type=oneshotExecStart=/usr/local/zabbix/sbin/zabbix_server -c /usr/local/zabbix/etc/zabbix_server.confExecReload=/usr/local/zabbix/sbin/zabbix_server -R config_cache_reloadRemainAfterExit=yesPIDFile=/var/run/zabbix/zabbix_server.pid[Install]WantedBy=multi-user.target 12345678910111213[Unit]Description=Zabbix AgentdAfter=syslog.target network.target mysqld.service[Service]Type=oneshotExecStart=/usr/local/zabbix/sbin/zabbix_agentd -c /usr/local/zabbix/etc/zabbix_agentd.confExecReload=/usr/local/zabbix/sbin/zabbix_agentd -R config_cache_reloadRemainAfterExit=yesPIDFile=/var/run/zabbix/zabbix_agentd.pid[Install]WantedBy=multi-user.target 1sed -i &quot;s/zabbix-agentd/zabbix_agentd/g&quot; zabbix-agentd.service 启动zabbix服务在zabbix安装目录下面可以直接启动 123#/usr/local/zabbix/sbin/zabbix_server starttcp 0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 7140/zabbix_agentd 设置开启自动启动1vim /etc/rc.d/rc.local 最后添加下面两行 12/usr/local/zabbix/sbin/zabbix_server start/usr/local/zabbix/sbin/zabbix_agentd start 至此，zabbix server端的安装完毕，我们可以通过浏览器来访问! Zabbix主要的配置文件两个:“zabbix_server.conf”负责服务器端的设定； zabbix_agent.conf”用来设置客户端代理参数；“zabbix_proxy.conf”用来设定分布式的部署。 Zabbix_server.conf参数除了保证服务正常运行外还涉及该服务器的性能，如果参数设定不合理可能会导致zabbix添加主机不正常、代理端数据无法正常收集或是zabbix服务器性能严重下降，经常报告CPU占用过高或是IO占用过高等问题。 Zabbix前端已经就绪！默认的用户名是Admin，密码是zabbix。 跳坑 从布署包安装 zabbix的时候，影响到了mysql.导导致mysql启动不起来！运行下面的命令之后，即可！ 1/usr/local/mysql/bin/mysqld_safe 还有就是布署包默认运行的是appace容器，但是我的服务器上面已经有nginx了。所以最后还是手动编译一下。 apt-get install zabbix-server-mysql zabbix-frontend-php 通过这种方式安装zabbix的时候，出现了一个问题就是重启lnmp 时，mysql不能正常启动 3.configure: error: Invalid Net-SNMP directory - unable to find net-snmp-config 解决方法1apt-get install libsnmp-dev 编译安装zabbix error: MySQL library not found 123find / -name &quot;mysql_config*&quot;/usr/local/mysql/bin/mysql_config 我把–with-mysql改成 1--with-mysql=/usr/local/mysql/bin/mysql_config 正常通过 15. PHP Parse error: syntax error, unexpected &apos;[&apos; in /var/www/html/index.php on line 29 1PHP 5.4 is required 1Get value from agent failed: cannot connect to [[127.0.0.1]:10050]: [111] Connection refused 解决方法： 12127.0.0.1 是我的zabbix server服务器，本身也有监控自己本身的agent功能。出现这种错误是因为忘记在zabbix服务器开户zabbix_agentd。 123find / -name *agentd.logservice zabbix-server restart 7.1Zabbix agent on Zabbix server is unreachable for 5 minutes 修改agent的配置文件，将ServerActive的地址改为zabbix-server的IP地址，重启zabbix_agent 在troubleshouting查看服务日志的时候，可以将注意力集中在有显示“fail”或者“Error”这类失败的关键词上，这样可以快速排错，找到问题的原因，而不必通篇阅读所有的日志，极大的提高效率。 作为运维工程师，脑袋储存的信息可能比较多、杂，时而出现忘记了某个服务、配置文件的绝对路径，如果记得文件或者目录的完整名，可以使用“locate+文件名”命令来定位文件的绝对路径，若是连文件名也记不大清了，没关系，还可以用Linux平台强大的搜索命令find，以全局查找的方式，通过星号来匹配到想要查找的文件的绝对路径，例如：find / -name *agentd.conf （从/目录开始，全局搜索以agentd结尾的.conf文件）。 这些都是作为一名运维工程师应该具备的基本技能，而不必通过死记硬背的方式来记忆所有文件的绝对路径。 参考文档1 zabbix-3.0.4安装部署 官方从部署包安装zabbix Installing Zabbix 3.0 on Ubuntu 16.04 在Ubuntu 上安装 Zabbix Zabbix Documentation 3.4 Zabbix agent on Zabbix server is unreachable for 5 minutes]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[puppet系统配置自动化解决方案(转)]]></title>
    <url>%2F2017%2F12%2F10%2Fpuppet%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E5%8C%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[相信做过运维的朋友都会有这样的体会：把一个新的服务器从刚装好系统的状态配置到可以运行应用程序，是个挺麻烦的过程。 就拿一个运行 nginx + php 的 web 服务器为例，可能需要部署 ssh 公钥，设置用户 sudo 权限，关闭密码登录、root 远程登录，配置 iptables 规则。然后安装所需版本的 nginx、 php 到规范的路径，不能搞错版本，以免缺失所需特性或者造成冲突，还要安装应用所需要的 php 扩展比如 gd 之类。 然后是用于监控的客户端程序，比如 nagios 的 nrpe ，或者 zabbix_agent，用于日志轮转的 cronolog 等等。最后还得记得修改 ulimits 、tcp 相关的内核参数。然后还得一一验证所有的设置是否正确。如果是部署了新的东西或者更新了配置，还得写一下安装文档，这样下回安装的时候才不会遗漏什么编译 选项、软件包，或者少设置了系统参数导致故障。 有人请假或则离职的时候，别人也才能接替。需要部署的还不会仅仅是 web 服务器，还有 lvs 、mysql、memcache，也许还会有 redis、sphinx 等等等等。有软件包需要升级，也得对所有用到的机器都升级一遍。更要命的是，往往需要配置的还不是一台机器，而是十几台乃至上百台。 这里可以看出，系统配置本身是件很繁琐的事情。 需要考虑到很多方面的事情，任何一个地方出了纰漏就可能导致故障或者埋下隐患。手动来做很容易出错，也很不够敏 捷高效，难以快速响应需求。为了解决这些问题，我也曾经用 bash 做过一些脚本，来实现部署和系统设置的自动化，但是受到 bash 的表达能力的限制，脚本的编写测试维护并不轻松。 我们碰到的麻烦别人也会碰到。所以就有了自动化好在现在有了一个好用的工具来解决这些问题。 puppet就是这样一个功能强大的系统配置集群管理工具。puppet分为 master端和agent 端，可以实现分布式分发，有强大的配置管理功能，可以实现自动化分发文件、安装软件包、执行命令、添加系统用户、设置 crontab 等等。它的配置文件是一种表达能力强的 DSL，可读性好、容易复用，且本身就可以作为很好的文档，这就免除了维护文档的负担，也避免了文档过时的问题。 puppet还使用了ssl 来保证通讯的安全性，防止敏感的配置信息泄露。支持集群化，以实现大批量主机并行更新维护。引入的 factor 还实现了针对不同系统、不同发行版、不同环境的针对性设置。以及很多方便强大的功能。 我们先看看如何安装 puppet 。最方便的方法是使用包管理器。对于 centos 等 redhat 系发行版，可以通过 yum 来安装。先把 puppet 的官方源加入到系统中： 1234567891011121314# cat &gt; /etc/yum.repos.d/puppet.repo &lt;&lt; EOF[puppet-dependencies]name=puppet dependenciesbaseurl=http://yum.puppetlabs.com/el/\$releasever/dependencies/\$basearch/gpgcheck=1gpgkey=http://yum.puppetlabs.com/RPM-GPG-KEY-puppetlabs[puppet-products]name=puppet dependenciesbaseurl=http://yum.puppetlabs.com/el/\$releasever/products/\$basearch/gpgcheck=1gpgkey=http://yum.puppetlabs.com/RPM-GPG-KEY-puppetlabsEOF 然后 1yum install puppet 就会自动安装好 puppet。从所列出依赖的软件包可以看出， puppet 是用 ruby 实现的。对于 debian 系发行版，也可以在 apt.puppetlabs.com 中找到相应的源和 gpg key。 puppet 分为 master 和 agent 。找两台机器或者两个虚拟机，一台作为 master，一台作为 agent ，两端都安装好后就可以开始配置了。 首先配置 master 。先在 master 端运行一下 1puppet master 初次运行会生成puppet 用户， 在 /etc/puppet 目录下生成默认配置目录结构并在 /var/lib/puppet 生成数据文件。如果提示 permisiton denided ，可以试试 chown puppet:puppet /var/lib/puppet/run 。 然后我们就可以开始写第一个配置文件了。在 /etc/puppet/manifests 目录下建一个 site.pp ，这是 puppet master 的主配置文件。输入 1234567package &#123; &quot;bison&quot;: ensure=&gt;&quot;installed&quot;,&#125;exec &#123; &quot;puppet test&quot;: command=&gt;&quot;/bin/touch /tmp/puppet-test&quot;,&#125; 第一个配置会使 agent 端确保编译 php 所必须的软件包 bison 已经安装好。对于不同的系统，会使用各自的包管理器来安装。第二个配置会在 agent 端执行 /bin/touch /tmp/puppet-test 。 然后配置客户端。先编辑 /etc/hosts，加入 master 的 ip，如： 1192.168.1.101 puppet 在客户端运行一下 1puppet agent --test 初次运行也同样会生成客户端的相应文件，然后就会去连接 master 端执行任务。此时会提示 1warning: peer certificate won&apos;t be verified in this SSL session exiting; no certificate found and waitforcert is disabled 这表示 agent 需要认证。因为 puppet 使用了 ssl 来保证安全，并需要 agent 经过 master 认证才能够访问配置。到 master 端执行一下 1puppet cert list 会列出待认证的 agent 列表。这里可以看到 agent 的主机名。如 1puppet-agent-test-01 (66:62:5C:84:B0:23:73:FB:80:7C:89:48:4C:A6:AF:53) 然后可以使用 1puppet cert sign puppet-agent-test-01 就能完成认证。如果觉得直接使用主机名不够灵活，也可以在运行 agent 时使用 –certname=认证名 来指定。在 agent 端再试一次，这回就可以看到，agent 已经开始干活了。看看 bison 工具是否安装好了，再看看 /tmp 目录下是否生成了 /tmp/puppet-test 文件。 需要注意的是，一个主机可以使用多个不同的certname，但一个certname只能被一台主机使用。如果原有的certname需要移动到另一个主机上使用，就需要在master端先 puppet cert clean “ 认证名” 来清除原有数据。所以，certname应当尽量保持全局唯一。 这里 agent 使用的 –test 让 agent 不以服务方式运行，只执行一次，并输出详细信息。去掉这个参数，puppet agent 就会以服务方式在后台运行，默认每 30 分钟连接一次服务器更新配置。可以用 puppet help master、puppet help agent 查看更多选项。 刚才是把所有的配置都写在了 sites.pp 文件里。在配置项增多，维护的项目增多以后，就会变得过于庞大而难以维护。所以就需要把配置分到不同的模块中去，以模块化的方式来管理配置。 puppet 的模块放在 /etc/puppet/modules 下。模块的目录结构如下图所示： 1234567modules/|-- test |-- files | `-- test.txt `-- manifests |-- init.pp `-- test.pp 在这个例子里，定义了一个 test 模块。其中 files 目录中用于安放该模块所需分发的文件，manifests 目录中是该模块的配置文件。其中 init.pp 是每个模块的主配置文件。内容通常为 import “*” ，来载入该模块的其他配置文件。我们在 files 目录中加入一个 test.txt 文件，并把之前 site.pp 中的内容挪到 test.pp 中，再加入分发文件的配置，定义成一个类： 123456789101112class test1 &#123;package &#123; &quot;bison&quot;: ensure=&gt;&quot;installed&quot;,&#125;exec &#123; &quot;puppet test&quot;: command=&gt;&quot;/bin/touch /tmp/puppet-test&quot;,&#125;file &#123; &quot;/tmp/test.txt&quot;: ensure =&gt; &quot;present&quot;, source =&gt; &quot;puppet:///modules/test/test.txt&quot;&#125;&#125; site.pp 中删除原有的配置，加入 import “test” ，把 test 模块加载进来，然后加入 include “test1” ，应用 test1 类的配置。Include语句也可以再class内使用。以在一个class中复用另一个class的配置。现在，我们在 agent 端再运行一次 puppet agent –test ，agent 还是会照常工作，但是配置已经分到模块中了。实践中，会把每个配置的项目建立一个模块，比如：nginx、php 等等。再看看 /tmp 目录，会发现 master 端的 test.txt 文件已经下载回来。puppet 已经完成了文件分发的工作。module 中的 files 通常用于分发配置文件，把软件包的配置文件集中管理。 file配置也可以用来创建目录。只要使用 ensure =&gt; “directory” 即可。如： 123file &#123; &quot;/tmp/testdir&quot;: ensure =&gt; &quot;directory&quot;,&#125; 到目前为止，我们只使用了一个agent，实际环境中，会有许多台需要不同配置的 agent 。这就需要对不同的 agent 应用不同的配置。在 sites.pp 中把 include test1 替换成针对特定节点的配置： 1234import &quot;test&quot;node &quot;puppet-agent-test-01&quot; &#123; include &quot;test1&quot;&#125; 这里的主机名可以用 “,” 分隔，指定多个主机，也可以用类似 /puppet-agent-test-.*/ 这样的正则表达式来灵活匹配。为了测试配置是否生效，我们可以修改一下之前的配置，再运行一下 agent。还可以再加入几台 agent 试试应用不同的节点的不同配置。 之前加入 agent 时，需要在 master 端手动为 agent 认证。在客户端众多，或者需要完全自动化的时候，可以配置自动签名。当然，前提是能通过别的途径比如 iptables 限制访问 master 的来源，或者是在可信的内网环境下。 在 /etc/puppet 目录中添加 autosign.conf 中输入agent 认证名的模式，如 .test.net 。需要注意的是，这里必须使用类似泛域名通配符的方式。也就是说， 只能出现在前面，而不允许 test.* 这样的形式。所以要应用自动签名，在规划 agent 的认证名的时候就要注意这一点。现在，我们用 puppet agent –test –certname=”agent01.test.net” 试试。这回就不再需要手动认证，直接就能执行 master 分发的配置任务了。 实际应用puppet时，会把puppet的配置文件，以及要分发的软件包的配置文件都加入到svn等源代码管理中。但是我们也会需要用puppet来分发一些我们自己编译打包的软件包等二进制文件。这些二进制文件并不适合放进源代码管理中。另外，需要用puppet分发的证书、密钥等敏感信息也不适合放入。这时，使用模块的文件就不太方便。好在puppet的文件服务器也是可以配置的。建立puppet文件服务器配置文件：/etc/puppet/fileserver.conf，输入 123456[modules] allow *[files] path /data/puppet allow * 这就定义了一个名为files的额外文件服务挂载点，位于 /data/puppet。也放一个 test.txt 在里面，然后就可以使用 123file &#123; &quot;/tmp/test2.txt&quot;: source =&gt; &quot;puppet:///files/test.txt&quot;&#125; 来分发了。这里 puppet:// 是协议名，后面的路径 /files/test.txt 的第一部分是挂载点名称。之前使用的 modules 这个特殊的挂载点名是指向各个模块的文件。如 /modules/test/test.txt 就是test模块下files目录中的test.txt文件。而 /files/test.txt 就是 files 挂载点对应目录下的 test.txt 文件。之后，我们就可以把这些二进制文件等用别的途径部署到自定义挂载点上。 之前我们使用的file、exec、package 在puppet中都被称为资源。每一种资源都有许多参数可以设置。对于 file 资源，可以用 ensure =&gt; “link”, target =&gt; “目标路径” 来建立软链接，用mode=&gt;0644来设置文件访问权限。对于exec资源，可以用cwd参数来设置当前路径，用creates参数来设置执行命令创建的路径，可以用于防止重复执行命令。对于所有的资源类型，都有一些共有的参数，称为元参数。其中最常用的就是require参数。这个参数指定了这个资源所依赖的资源。实际应用中，不同的任务间会有依赖关系。比如安装软件包需要先创建好目标路径的目录，需要先安装好所需的依赖软件包，这就可以用require选项来实现。比如： 12345678exec &#123; &quot;install sth.&quot;: cwd =&gt; &quot;/opt/some_package&quot;, exec =&gt; &quot;/bin/tar -xzvf /path/to/package.tar.gz&quot;, require =&gt; File[&quot;/opt/some_package&quot;],&#125;file &#123; &quot;/opt/some_package&quot;: ensure =&gt; &quot;directory&quot;,&#125; 任务 Exec[“install sth.”] 就会在任务 File[“/opt/some_package”]完成后运行。这里，对于每个类型的资源，可以用“，”分隔；如果要指定多种类型的资源，也可以写成列表形式。如： 1require =&gt; [ File[&quot;/path/to/file1&quot;, &quot;/path/to/file2&quot;], Package[&quot;package1] ] 还有一个与require相反的参数before，可以指定该任务必须在哪些任务前完成。 puppet还提供了多种资源类型来完成不同的任务。比如可以用cron类型来管理定时任务，用host类型来设置hosts文件等。 至此，已经简单介绍了puppet 的基本功能设置，可以针对不同主机安装执行不同的所需任务。 puppet 还有更多强大的功能，您可以参照 puppet 的官方文档，各取所需，在实践中学习应用。 应用puppet，把原先繁琐的系统配置过程自动化了，需要部署一台新的服务器时，只需要在初始化好puppet后，执行一次 puppet agent –test，剩下的就交给它来干了。既省时省力也不会出错。 机械化的操作就应当交给机器来做，这样才能把人的精力省出来做更有价值的事情。 原文引自]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>puppet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL搭建主从复制实现细节分析]]></title>
    <url>%2F2017%2F12%2F09%2FMySQL%E6%90%AD%E5%BB%BA%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[概念 主从复制可以使MySQL数据库主服务器的主数据库，复制到一个或多个MySQL从服务器从数据库，默认情况下，复制异步; 根据配置，可以复制数据库中的所有数据库，选定的数据库或甚至选定的表。 MySQL中主从复制的优点横向扩展解决方案在多个从库之间扩展负载以提高性能。在这种环境中，所有写入和更新在主库上进行。但是，读取可能发生在一个或多个从库上。该模型可以提高写入的性能（由于主库专用于更新），同时在多个从库上读取，可以大大提高读取速度。 数据安全性由于主库数据被复制到从库，从库可以暂停复制过程，可以在从库上运行备份服务，而不会破坏对应的主库数据。 分析可以在主库上创建实时数据，而信息分析可以在从库上进行，而不会影响主服务器的性能。 长距离数据分发可以使用复制创建远程站点使用的数据的本地副本，而无需永久访问主库。 1.准备工作Mysql版本：MySQL 5.7.11Master-Server : 192.168.252.123Slave-Server : 192.168.252.124 关闭防火墙 1systemctl stop firewalld.service 安装 MySQL 首先在两台机器上装上，保证正常启动，可以使用 Master-Server 配置修改 my.cnf 配置 Master 以使用基于二进制日志文件位置的复制，必须启用二进制日志记录并建立唯一的服务器ID,否则则无法进行主从复制。 停止MySQL服务。 1service mysql.server stop 开启binlog ，每台设置不同的 server-id 1234$ cat /etc/my.cnf[mysqld]log-bin=mysql-binserver-id=1 启动MySQL服务 1service mysql.server start 登录MySQL 1/usr/local/mysql/bin/mysql -uroot -p 创建用户每个从库使用MySQL用户名和密码连接到主库，因此主库上必须有用户帐户，从库可以连接。任何帐户都可以用于此操作，只要它已被授予 REPLICATION SLAVE权限。可以选择为每个从库创建不同的帐户，或者每个从库使用相同帐户连接到主库 虽然不必专门为复制创建帐户，但应注意，复制用到的用户名和密码会以纯文本格式存储在主信息存储库文件或表中 。因此，需要创建一个单独的帐户，该帐户只具有复制过程的权限，以尽可能减少对其他帐户的危害。 登录MySQL 1/usr/local/mysql/bin/mysql -uroot -p 12mysql&gt; CREATE USER &apos;replication&apos;@&apos;192.168.252.124&apos; IDENTIFIED BY &apos;mima&apos;;mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;replication&apos;@&apos;192.168.252.124&apos;; Slave-Server 配置修改 my.cnf 停止MySQL服务。 1service mysql.server stop 123$ cat /etc/my.cnf[mysqld]server-id=2 如果要设置多个从库，则每个从库的server-id与主库和其他从库设置不同的唯一值。 启动MySQL服务 1service mysql.server start 登录MySQL 配置主库通信 查看 Master-Server ， binlog File 文件名称和 Position值位置 并且记下来 1mysql&gt; show master status; 要设置从库与主库进行通信，进行复制，使用必要的连接信息配置从库在从库上执行以下语句将选项值替换为与系统相关的实际值 参数格式，请勿执行 123456mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST=&apos;master_host_name&apos;, -&gt; MASTER_USER=&apos;replication_user_name&apos;, -&gt; MASTER_PASSWORD=&apos;replication_password&apos;, -&gt; MASTER_LOG_FILE=&apos;recorded_log_file_name&apos;, -&gt; MASTER_LOG_POS=recorded_log_position; 1234567mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST=&apos;192.168.252.123&apos;, -&gt; MASTER_USER=&apos;replication&apos;, -&gt; MASTER_PASSWORD=&apos;mima&apos;, -&gt; MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, -&gt; MASTER_LOG_POS=629;Query OK, 0 rows affected, 2 warnings (0.02 sec) MASTER_LOG_POS=0 写成0 也是可以的 放在一行执行方便 1CHANGE MASTER TO MASTER_HOST=&apos;192.168.252.123&apos;, MASTER_USER=&apos;replication&apos;, MASTER_PASSWORD=&apos;mima&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, MASTER_LOG_POS=629; 启动从服务器复制线程 12mysql&gt; START SLAVE;Query OK, 0 rows affected (0.00 sec) 查看复制状态 检查主从复制通信状态 Slave_IO_State #从站的当前状态Slave_IO_Running： Yes #读取主程序二进制日志的I/O线程是否正在运行Slave_SQL_Running： Yes #执行读取主服务器中二进制日志事件的SQL线程是否正在运行。与I/O线程一样Seconds_Behind_Master #是否为0，0就是已经同步了 必须都是 Yes 在mysql5.0以后的版本，mysql主从已经相当的成熟了，可以只监控Slave_IO_Running，Slave_SQL_Running，Seconds_Behind_Master状态就可以了，这里不再做说明。 测试主从复制 启动MySQL服务 登录MySQL 在 Master-Server 创建测试库 123mysql&gt; CREATE DATABASE `replication_wwww.ymq.io`;mysql&gt; use `replication_wwww.ymq.io`;mysql&gt; CREATE TABLE `sync_test` (`id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8; 在 Slave-Server 查看是否同步过来 123mysql&gt; show databases;mysql&gt; use replication_wwww.ymq.iomysql&gt; show tables; 一些坑 Last_IO_Errno: 1045错误： 解决方案 Connecting and Last_IO_Errno: 2003 错误 解决方案2 Last_Errno: 1008解决方案3 MySQL错误处理–1146错误 解决方案 Last_SQL_Errno：1062 解决方案 参考文档1 mysql主主和主主集群 双主同步，如果服务器意外挂机 MySQL基于日志（binlog）主从复制搭建 MySQL数据库设置主从同步]]></content>
      <categories>
        <category>安全运维</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu配置Shadowsocks实现终端代理]]></title>
    <url>%2F2017%2F12%2F09%2FUbuntu%E9%85%8D%E7%BD%AEShadowsocks%E5%AE%9E%E7%8E%B0%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景场景一： 这几天想配置PHP Laravel框架，Laravel框架需要Composer安装。结果安装Composer的时候遭遇到了GFW，光在浏览器上穿墙还不够，还要在终端上穿墙。使用Shadowsocks在浏览器上穿墙很简单，但是在终端穿墙以前没接触过，这次花了5个小时搞定了。 利用linode翻墙安装相关配置时，哪速度，真是让人怀念。所以在局域网的一台机器上面开始动手了。 场景二： 做开发的同学，应该都会经常接触终端，有些时候我们在终端会做一些网络操作，比如下载gradle包等，由于一些你懂我也懂的原因，某些网络操作不是那么理想，这时候我们就需要设置代理来自由地访问网络。 Shadowsocks是我们常用的代理工具，它使用socks5协议，而终端很多工具目前只支持http和https等协议，对socks5协议支持不够好，所以我们为终端设置shadowsocks的思路就是将socks协议转换成http协议，然后为终端设置即可。仔细想想也算是适配器模式的一种现实应用吧。 想要进行转换，需要借助工具，这里我们采用比较知名的polipo来实现。 polipo是一个轻量级的缓存web代理程序。 前题是，已经在服务器端安装好了相关应用。 Shadowsocks 是一个开源安全的 Socks5 代理，中文名称“影梭“，类似于 SSH 代理。与一度非常流行的基于 GAE 的科学上网方式相比，Shadowsocks 部署简单，使用灵活；同时与全局代理的 VPN 不同，Shadowsocks 可以仅针对浏览器代理，轻巧方便，如果说 VPN 是一把 ==屠龙宝刀==，那么 Shadowsocks 就是一把 ==瑞士军刀==，虽小巧但功能强大。 1.ubuntu安装shadowsocks运行环境安装123sudo apt-get updatesudo apt-get install python-pipsudo apt-get install python-setuptools m2crypto 接着安装shadowsocks 1pip install shadowsocks 如果是ubuntu16.04 直接 (16.04 里可以直接用apt 而不用 apt-get 这是一项改进） 2.启动shadowsocks安装好后，在本地我们要用到sslocal ，终端输入sslocal –help 可以查看帮助，像这样 通过帮助提示我们知道各个参数怎么配置，比如 sslocal -c 后面加上我们的json配置文件，或者像下面这样直接命令参数写上运行。 比如 1sslocal -s 11.22.33.44 -p 50003 -k &quot;123456&quot; -l 1080 -t 600 -m aes-256-cfb -s表示服务IP, -p指的是服务端的端口，-l是本地端口默认是1080, -k 是密码（要加””）, -t超时默认300,-m是加密方法默认aes-256-cfb， 为了方便我推荐直接用sslcoal -c 配置文件路径 这样的方式，简单好用。 我们可以在/home/mudao/ 下新建个文件shadowsocks.json (mudao是我在我电脑上的用户名，这里路径你自己看你的)。内容是这样：12345678&#123;&quot;server&quot;:&quot;11.22.33.44&quot;,&quot;server_port&quot;:50003,&quot;local_port&quot;:1080,&quot;password&quot;:&quot;123456&quot;,&quot;timeout&quot;:600,&quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 确定上面的配置文件没有问题，然后我们就可以在终端输入 1sslocal -c /etc/shadowsocks.json 回车运行。如果没有问题的话，下面会是这样… 3.开机后台自动运行ss如果你上面可以代理上网了可以进行这一步，之前我让你不要关掉终端，因为关掉终端的时候代理就随着关闭了，之后你每次开机或者关掉终端之后，下次你再想用代理就要重新在终端输入这样的命令 sslocal -c /home/mudao/shadowsocks.json ，挺麻烦是不？ 我们现在可以在你的ubuntu上安装一个叫做supervisor的程序来管理你的sslocal启动。关于supervisor在前面介绍安装mod-ss-panel时，有介绍！ 1sudo apt-get install supervisor 安装好后我们可以在/etc/supervisor/目录下找到supervisor.conf配置文件，我们可以用以下命令来编辑 1sudo gedit /etc/supervisor/supervisor.conf 在这个文件的最后加上以下内容 1234567[program:shadowsocks]command=sslocal -c /home/mudao/shadowsocks.jsonautostart=trueautorestart=trueuser=rootlog_stderr=truelogfile=/var/log/shadowsocks.log 现在关掉你之前运行sslocal命令的终端，再打开终端输入 1sudo service supervisor restart 然后去打开浏览器看看可不可以继续代理上网。你也可以用 1ps -ef|grep sslocal 命令查看sslocal是否在运行。 这个时候我们需要在/etc下编辑一个叫rc.local的文件 ，让supervisor开机启动。 1sudo gedit /etc/rc.local 在这个配置文件的 1exit 0 前面一行加上 1service supervisor start 保存。 看你是否配置成功你可以在现在关机重启之后直接打开浏览器看是否代理成功。 以上原文引自这里4. 终端穿墙浏览器能穿墙就已经能满足绝大多数需求了，但是有的时候终端也必须穿墙，就比如Composer。关于终端穿墙，本人尝试了很多种方案，比如Privoxy、Proxychains和Polipo，最后选择了Privoxy。 为什么终端需要单独穿墙呢？难道Shadowsock不能“全局”代理么？这个问题当时困惑了我很久，最后一句话点醒了我。 Shadowsocks是一个使用SOCKS5（或者SOCK4之类）协议的代理，它只接受SOCKS5协议的流量，不接受HTTP或者HTTPS的流量。所以当你在Chrome上能穿墙的时候，是Proxy SwitchyOmega插件把HTTP和HTTPS流量转换成了SOCKS协议的流量，才实现了Shadowsocks的代理。而终端是没有这样的协议转换的，所以没法直接使用Shadowsock进行代理。 这时候就需要一个协议转换器，这里我用了Privoxy(我用privoxy没有成功！但是用polipo成功了)。 1~$ sudo apt-get install privoxy 安装好后进行配置，Privoxy的配置文件在/etc/privoxy/config，这个配置文件中注释很多。 找到 1listen-address 这一节，确认监听的端口号(这个端口号要跟1080区分开来，之前没有成功。估计就是因为把这个端口号改了)。 找到5.2. forward-socks4, forward-socks4a, forward-socks5 and forward-socks5t这一节，加上如下配置，注意最后的点号。 有关Privoxy的配置就结束了，重启一下Privoxy。 1~$ sudo /etc/init.d/privoxy restart 接着配置一下终端的环境，需要如下两句。 12~$ export http_proxy=&quot;127.0.0.1:8118&quot;~$ export https_proxy=&quot;127.0.0.1:8118&quot; 为了方便还是在/etc/rc.local中添加如下命令，注意在exit 0之前。 1sudo /etc/init.d/privoxy start 在/etc/profile的末尾添加如下两句。 12export http_proxy=&quot;127.0.0.1:8118&quot;export https_proxy=&quot;127.0.0.1:8118&quot; 安装privoxy的参考原文在这里 5.Shadowsocks 转换 HTTP 代理(使用Polipo)Shadowsocks 默认是用 Socks5 协议的，对于 ==Terminal== 的 get,wget 等走 Http 协议的地方是无能为力的，所以需要转换成 Http 代理，加强通用性，这里使用的转换方法是基于 Polipo 的。 输入命令安装 Polipo： 1sudo apt-get install polipo 修改配置文件： 1sudo gedit /etc/polipo/config 将下面的内容整个替换到文件中并保存： 123456789101112131415161718# This file only needs to list configuration variables that deviate# from the default values. See /usr/share/doc/polipo/examples/config.sample# and &quot;polipo -v&quot; for variables you can tweak and further information.logSyslog = falselogFile = &quot;/var/log/polipo/polipo.log&quot; socksParentProxy = &quot;127.0.0.1:1080&quot;socksProxyType = socks5 chunkHighMark = 50331648objectHighMark = 16384 serverMaxSlots = 64serverSlots = 16serverSlots1 = 32 proxyAddress = &quot;0.0.0.0&quot;proxyPort = 8123 重启 Polipo： 1/etc/init.d/polipo restart 验证代理是否正常工作： 12export http_proxy=&quot;http://127.0.0.1:8123/&quot;curl www.google.com 如果正常，就会返回抓取到的 Google 网页内容。 第二种验证代理是否正常工作的方法： 安装完成就需要进行验证是否work。这里展示一个最简单的验证方法，打开终端，如下执行 123407:56:24-androidyue/var/log$ curl ip.gs当前 IP：125.39.112.15 来自：中国天津天津 联通08:09:23-androidyue/var/log$ http_proxy=http://localhost:8123 curl ip.gs当前 IP：210.140.193.128 来自：日本日本 如上所示，为某个命令设置代理，前面加上http_proxy=http://localhost:8123 后接命令即可。注：8123是polipo的默认端口，如有需要，可以修改成其他有效端口。 当前会话全局设置如果嫌每次为每一个命令设置代理比较麻烦，可以为当前会话设置全局的代理。 即使用 1export http_proxy=http://localhost:8123 即可。 如果想撤销当前会话的http_proxy代理，使用 1unset http_proxy 1234567821:29:49-androidyue~$ curl ip.gs当前 IP：125.39.112.14 来自：中国天津天津 联通21:29:52-androidyue~$ export http_proxy=http://localhost:812321:30:07-androidyue~$ curl ip.gs当前 IP：210.140.193.128 来自：日本日本 21:30:12-androidyue~$ unset http_proxy21:30:37-androidyue~$ curl ip.gs当前 IP：125.39.112.14 来自：中国天津天津 联通 如果想要更长久的设置代理，可以将 12export http_proxy=http://localhost:8123export https_proxy=http://localhost:8123 加入.bashrc或者.bash_profile文件 另外，在浏览器中输入 1http://127.0.0.1:8123/ 便可以进入到 Polipo 的使用说明和配置界面。 设置浏览器和开机启动 最后就是将转换后的 Http 代理设置到浏览器中，地址是 127.0.0.1，端口 8123，代理类型当然是选择 Http 啦。对于 FireFor 用户来说，插件可以选择 AutoProxy 或 FoxyProxy 配置polipo在原文在这里 引申： 6.设置Git代理（接上面的polipo）复杂一些的设置Git代理 12345678git clone https://android.googlesource.com/tools/repo --config http.proxy=localhost:8123Cloning into &apos;repo&apos;...remote: Counting objects: 135, doneremote: Finding sources: 100% (135/135)remote: Total 3483 (delta 1956), reused 3483 (delta 1956)Receiving objects: 100% (3483/3483), 2.63 MiB | 492 KiB/s, done.Resolving deltas: 100% (1956/1956), done. 其实这样还是比较复杂，因为需要记忆的东西比较多， 下面是一个更简单的实现 首先，在.bashrc或者.bash_profile文件加入这一句。 1gp=&quot; --config http.proxy=localhost:8123&quot; 然后 执行source操作，更新当前bash配置。 更简单的使用git的方法 12345678git clone https://android.googlesource.com/tools/repo $gpCloning into &apos;repo&apos;...remote: Counting objects: 135, doneremote: Finding sources: 100% (135/135)remote: Total 3483 (delta 1956), reused 3483 (delta 1956)Receiving objects: 100% (3483/3483), 2.63 MiB | 483 KiB/s, done.Resolving deltas: 100% (1956/1956), done. [在git终端mac终端加入代理原文引自这里]http://droidyue.com/blog/2016/04/04/set-shadowsocks-proxy-for-terminal/) 7.apt-get怎么使用代理服务器升级到Ubuntu10.04后，发现apt-get的代理设置有改变了，在9.10以前使用“http_proxy”环境变量就可以令apt-get使用代理. 然后在Ubuntu10.04下就无效了，看来apt-get已经被改成不使用这个环境变量了。 一阵郁闷后，最后我发现在“首选项”-&gt;“网络代理”那里，多了个“System-wide”按钮（我用的是英文环境，不知道中文被翻译成怎样，关闭窗口时也会提示你），在这里设置后，apt-get确实可以使用代理了。 但是我依然鄙视这种改进，因为我通常就是偶尔使用代理，更新几个被墙掉的仓库而已（如dropbox和tor），根本不想使用全局代理，本来用终端就能搞定的事，现在切换代理要点N次鼠标，真烦。 所以我研究了一下，发现那个代理设置修改了两个文件，一个是“/etc/environment”，这个是系统的环境变量，里面定义了“http_proxy”等代理环境变量。另一个是“/etc/apt/apt.conf”，这个就是apt的配置，内容如下 在/etc/apt/apt.conf中追加 123Acquire::http::proxy &quot;http://127.0.0.1:8123/&quot;;Acquire::ftp::proxy &quot;ftp://127.0.0.1:8123/&quot;;Acquire::https::proxy &quot;https://127.0.0.1:8123/&quot;; 很明显的代理设置代码，我看了下apt-get的手册，发现可以用“-c”选项来指定使用配置文件，也就是复制一份为“~/apt_proxy.conf”，然后“网络代理”那里重置回直接连接，以后使用 1sudo apt-get -c ~/apt_proxy.conf update 1sudo apt-get -c ~/apt_proxy.conf install mongodb apt-get 使用代理在的原文在这里]]></content>
      <categories>
        <category>科学上网</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo + Github Pages搭建个人独立博客]]></title>
    <url>%2F2017%2F12%2F08%2F%E4%BD%BF%E7%94%A8Hexo-Github-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%8B%AC%E7%AB%8B%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[系统环境配置要使用Hexo，需要在你的系统中支持Nodejs以及Git 安装Hexo1234567$ cd d:/hexo$ npm install hexo-cli -g$ hexo init blog$ cd blog$ npm install$ hexo g # 或者hexo generate$ hexo s # 或者hexo server，可以在http://localhost:4000/ 查看 这里有必要提下Hexo常用的几个命令：12$ hexo new &quot;postName&quot; #新建文章$ hexo new page &quot;pageName&quot; #新建页面 常用简写1234$ hexo n == hexo new$ hexo g == hexo generate$ hexo s == hexo server$ hexo d == hexo deploy 123hexo generate (hexo g) 生成静态文件，会在当前目录下生成一个新的叫做public的文件夹hexo server (hexo s) 启动本地web服务，用于博客的预览hexo deploy (hexo d) 部署播客到远端（比如github, heroku等平台） 常用组合12$ hexo d -g #生成部署$ hexo s -g #生成预览 Hexo主题设置 Github Pages设置 什么是Github Pages GitHub Pages 本用于介绍托管在GitHub的项目，不过，由于他的空间免费稳定，用来做搭建一个博客再好不过了。 每个帐号只能有一个仓库来存放个人主页，而且仓库的名字必须是username/username.github.io，这是特殊的命名约定。你可以通过http://username.github.io 来访问你的个人主页。 这里特别提醒一下，需要注意的个人主页的网站内容是在master分支下的。 部署Hexo到Github Pages 首先需要明白所谓部署到github的原理。 之前步骤中在Github上创建的那个特别的repo（jiji262.github.io）一个最大的特点就是其master中的html静态文件，可以通过链接http://jiji262.github.io来直接访问。 Hexo -g 会生成一个静态网站（第一次会生成一个public目录），这个静态文件可以直接访问。需要将hexo生成的静态网站，提交(git commit)到github上。明白了原理，怎么做自然就清晰了. 使用hexo deploy部署hexo deploy可以部署到很多平台，具体可以参考这个链接. 如果部署到github，需要在配置文件_config.xml中作如下修改： 1234deploy: type: git repo: git@github.com:jiji262/jiji262.github.io.git branch: master 然后在命令行中执行 1hexo d 即可完成部署。 踩坑提醒 注意需要提前安装一个扩展： 1$ npm install hexo-deployer-git --save Hexo 主题配置参考文档1]]></content>
      <categories>
        <category>前端开发</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F12%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
